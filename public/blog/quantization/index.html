<!doctype html><html lang=en class=scroll-smooth><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Demystifying Quantization: Shrinking Models for Efficient AI | Bimal Timilsina</title>
<meta property="og:title" content="Demystifying Quantization: Shrinking Models for Efficient AI | Bimal Timilsina"><meta property="og:description" content="Quantization, the process of reducing model size by converting parameters to lower precision, addresses the challenges posed by large neural network models, particularly in deployment. This article explores the necessity of quantization, its methodologies like asymmetric and symmetric approaches, and the distinction between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)."><link rel=icon type=image/svg href=http://bimaltimilsina.com.np/favicon.svg hreflang=en-us><meta property="og:type" content="website"><meta property="og:url" content="http://bimaltimilsina.com.np/blog/quantization/"><meta property="og:site_name" content="Demystifying Quantization: Shrinking Models for Efficient AI | Bimal Timilsina"><meta property="og:image" content><meta property="og:image:width" content="1280"><meta property="og:image:height" content="800"><meta property="og:image:type" content="image/png"><meta property="og:locale" content="en_US"><meta name=description content="Quantization, the process of reducing model size by converting parameters to lower precision, addresses the challenges posed by large neural network models, particularly in deployment. This article explores the necessity of quantization, its methodologies like asymmetric and symmetric approaches, and the distinction between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)."><meta name=keywords content="development machine-learning bimal bimal-timilsina timilsina llm large-language-models rag"><meta itemprop=name content="Demystifying Quantization: Shrinking Models for Efficient AI | Bimal Timilsina"><meta itemprop=description content="Quantization, the process of reducing model size by converting parameters to lower precision, addresses the challenges posed by large neural network models, particularly in deployment. This article explores the necessity of quantization, its methodologies like asymmetric and symmetric approaches, and the distinction between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)."><meta itemprop=image content><meta name=twitter:card content="summary"><meta name=twitter:url content="http://bimaltimilsina.com.np/blog/quantization/"><meta name=twitter:site content="@timilsinabml05"><meta name=twitter:creator content="@timilsinabml05"><meta name=twitter:title content="Demystifying Quantization: Shrinking Models for Efficient AI | Bimal Timilsina"><meta name=twitter:description content="Quantization, the process of reducing model size by converting parameters to lower precision, addresses the challenges posed by large neural network models, particularly in deployment. This article explores the necessity of quantization, its methodologies like asymmetric and symmetric approaches, and the distinction between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)."><meta name=twitter:image:src content><link rel=canonical href=http://bimaltimilsina.com.np/blog/quantization/><link rel=robots href=/robots.txt><meta name=msapplication-TileColor content="#0f172a"><meta name=theme-color content="#0f172a"><meta name=next-head-count content="29"><script src=https://cdn.tailwindcss.com></script><link rel=stylesheet href=http://bimaltimilsina.com.np/css/style.css><script src=https://cdn.jsdelivr.net/npm/js-polyfills@0.1.43/polyfill.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js type=text/javascript></script><script src=https://unpkg.com/scrollreveal/dist/scrollreveal.min.js></script><script src=http://bimaltimilsina.com.np/js/site_main.js defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script></head><body class="bg-slate-900 leading-relaxed text-slate-400 selection:bg-teal-300 selection:text-teal-900"><div class="font-sans group/spotlight relative"><div id=glow_box class="pointer-events-none position-fixed inset-0 z-30 transition duration-300 lg:absolute"></div><main><div class="mx-auto max-w-3xl px-4"><article><div><header class="pt-16 md:pt-20 lg:pt-28 mb-10 md:mb-20"><a class="group mb-2 inline-flex items-center font-semibold leading-tight text-teal-300" href=http://bimaltimilsina.com.np/blog/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor" class="mr-1 h-4 w-4 rotate-180 transition-transform group-hover:-translate-x-2" aria-hidden="true"><path fill-rule="evenodd" d="M3 10a.75.75.0 01.75-.75h10.638L10.23 5.29a.75.75.0 111.04-1.08l5.5 5.25a.75.75.0 010 1.08l-5.5 5.25a.75.75.0 11-1.04-1.08l4.158-3.96H3.75A.75.75.0 013 10z" clip-rule="evenodd"/></svg>Blog</a><h1 class="text-3xl font-semibold tracking-tight leading-tight text-gray-900 md:text-4xl lg:text-4xl dark:text-white">Demystifying Quantization: Shrinking Models for Efficient AI</h1><div class="text-gray-400 mt-6"><time>Apr 26, 2024</time> — <span>24
mins read</span> — <a class="group mb-2 inline-flex items-center font-semibold leading-tight text-teal-300" href=http://bimaltimilsina.com.np/categories/large-language-models>Large Language Models</a></div></header><div class="prose md:prose-lg dark:prose-invert article prose-headings:font-semibold mt-6"><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>Large Language Models (LLMs) are revolutionizing AI research these days. Many tasks that once required complex models can now be solved in minutes with the help of LLMs. Not only are they generation models, but they can also tackle summarization, classification, question answering, clustering, and much more. While all these benefits are fantastic, using LLMs on your own machine can be challenging due to their size. Most LLMs require larger GPUs to run, which might be feasible for big companies but can be a stumbling block for individuals.</p><p>Enter quantization. Quantization is a method that significantly reduces the size of any model. In quantization, we convert the model&rsquo;s parameters from higher precision, like FLOAT32 (FP32), to lower precision, such as INT4 or INT8. This greatly shrinks the model&rsquo;s size. However, since we&rsquo;re reducing the precision of the model&rsquo;s parameters, there&rsquo;s a slight decrease in accuracy. But the trade-off in size might be well worth it.</p><h3 id=but-why-do-we-need-quantization class="relative group">But, why do we need Quantization? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#but-why-do-we-need-quantization aria-label=Anchor>#</a></span></h3><p>As I mentioned earlier, running an LLM with 100 billion parameters on your computer is not feasible, so we need a way to run these models on our machines without significant performance degradation. Models are usually trained in higher precision, i.e., FLOAT32, and when we quantize these models, we typically convert them to a lower precision range like INT8 or even INT4.</p><p>Let&rsquo;s use an LLM as an example. Llama3-70B has 70 billion parameters. To store this model in full precision, i.e., FP32, we need $70 \space billion \times 32 \space bit = \frac{70 \times 10^9 \times 32}{8 \times 10^9} = 280 \space GB$ of storage.</p><p>Now, let&rsquo;s also calculate the GPU memory (VRAM) required to run this 70 billion parameter model.</p><p>We have a formula.</p><p>$$ Memory \space required (M) \approx \frac{P \times 4 \space bytes}{ \frac{32}{Q} } \times 1.2 $$
$$\text{or} $$
$$\frac{P \times Q \times 4 \space bytes}{32} \times 1.2$$</p><p>$Where$,</p><table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td>$ M $</td><td>GPU memory expressed in Gigabyte</td></tr><tr><td>$P$</td><td>The amount of parameters in the model. E.g. a 7B model has 7 billion parameters.</td></tr><tr><td>$4 \space bytes$</td><td>4 bytes, expressing the bytes used for each parameter</td></tr><tr><td>$32$</td><td>There are 32 bits in 4 bytes</td></tr><tr><td>$Q$</td><td>The amount of bits that should be used for loading the model. E.g. 16 bits, 8 bits or 4 bits.</td></tr><tr><td>$1.2$</td><td>Represents a 20% overhead of loading additional things in GPU memory.</td></tr></tbody></table><p>$$ M \approx \frac{70 \times 4 \times 32}{32} \times 1.2 = 336 \space GB $$</p><p>So, the memory required to run the LLama3-70B in full precision is approximately 336 GB.</p><p>This is really high, since we need high end GPU or multiple GPUs to run this model in full precision. Even if we take smaller models like 7B or 8B, we need almost 28 GB VRAM.</p><p>This seems impossible for individual users. But if we quantize the same model in 4-bit or 8-bit we need much smaller VRAM. let&rsquo;s calculate:</p><p><em>For 4-bit</em>
$$ M \approx \frac{7 \times 4 \times 4}{32} \times 1.2 $$
$$M \approx 4.2 \space GB$$</p><p><em>For 8-bit</em>
$$ M \approx \frac{7 \times 8 \times 4}{32} \times 1.2 $$
$$M \approx 8.4 \space GB$$</p><p>Even after quantization, these models require much larger GPU RAM. But these are in acceptable range as we can run these models simply in our computer.</p><p>So, quantization helps us use these models on smaller GPU without much performance degradation.</p><h2 id=how-computer-stores-numbers class="relative group">How Computer Stores Numbers <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#how-computer-stores-numbers aria-label=Anchor>#</a></span></h2><p>Before delving into the details of how we quantize models from high precision to low precision, let&rsquo;s take a look at how computers store numbers in CPU or memory. For every bit of a number, the computer requires an equal amount of storage. For instance, if we have a 32-bit number, the computer will use 32 bits to store that number in memory.</p><p>Computers store numbers in two ways: Unsigned and Signed. A Signed value will store the sign (positive or negative) of the number, while an Unsigned value cannot store the sign of the number. To store the sign of any number, the computer uses 1 bit of memory.</p><p>Let&rsquo;s look an example on how a 8-bit Unsigned integer stored in memory.</p><table><thead><tr><th>1</th><th>0</th><th>0</th><th>0</th><th>1</th><th>0</th><th>0</th><th>1</th></tr></thead></table><p>$2^7 + 0 + 0 + 0 + 2^3 + 0 + 0 + 2^0 = 137$</p><p>The range of integers an 8-bit integer can store is calculated as: $range \space [0, 2^n-1] = [0, 255] $ for 8-bit unsisigned integers. This means that, it can only stores values from 0 to 255. If the value lies beside these boundaries, then they will be set as either 0 for smaller and 255 for larger value.</p><p>This is different for signed integers, as the range is calculated as:
$$range \space [- \space 2^{n-1}, 2^{n-1}-1]$$</p><p>For 8-bit signed integers the range will be $[-128, 127]$</p><p>This is completely different when it comes to floating point numbers. In floating point numbers, we will have three components:</p><ol><li>Sign</li><li>Exponent or Range</li><li>Fraction or Precision or Mantissa</li></ol><p>Below, you can see how computer stores floating point numbers. Each of these formats consume different chunk of the memory.
For example, float32 allocates 1 bit for sign, 8 bits for exponent and 23 bits for mantissa.</p><p>Similarly, float16 or FP16 allocates 1 bit for sign but just 5 bits for exponent and 10 bits for mantissa. On the other hand, BF16 (B stands for Google Brain) allocates 8 bits for the exponent and just 7 bits for mantissa.</p><p><em>FP 32 or Float32</em></p><p><figure><img src=/img/quantization/floating1.png alt=FP32 class="mx-auto my-0 rounded-md"></figure></p><p><em>FP 16 or Float16</em></p><p><figure><img src=/img/quantization/fp16.png alt=FP32 class="mx-auto my-0 rounded-md"></figure></p><p><em>BFLOAT16</em></p><p><figure><img src=/img/quantization/bfloat16.png alt=FP32 class="mx-auto my-0 rounded-md"></figure></p><p>So, in short the conversion from a higher memory format to a lower memory format is called quantization. Talking in deep learning terms, Float32 is referred to as <strong>single or full precision</strong> and Float16 and BFloat16 are called <strong>half precision</strong>. The default way in which deep learning models are trained and stored is in <strong>full precision</strong>. The most commonly used conversion is from full precision to an int8 and int4 format.</p><h2 id=types-of-quantization class="relative group">Types of Quantization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#types-of-quantization aria-label=Anchor>#</a></span></h2><p>When considering the types of quantization available for use during model compression, there are 2 main types to pick from</p><h3 id=asymmetric--quantization class="relative group">Asymmetric Quantization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#asymmetric--quantization aria-label=Anchor>#</a></span></h3><p>In asymmetric mode, we map min-max of dequantized value to min-max range of target precision. This is done by using zero-point also called quantization bias. Here zero point can be non-zero value.</p><p>$$X_q = clamp \left( \frac{X}{S} + Z ; 0, 2^n - 1\right)$$
Where,
$$
clamp(x;a,c) = \begin{cases}
a &\quad x &lt; a \\
x &\quad a \le x \le c \\
c &\quad x>c
\end{cases}
$$</p><p>$$ S = \frac{X_{max} - X_{min}}{2^n - 1} $$</p><p>$$Z = -\frac{X_{min}}{S}$$</p><p>Here,</p><p>$X \quad = $ Original floating point tensor<br>$X_q \quad = $ Quantized Tensor<br>$S \quad=$ Scale Factor<br>$Z \quad= $ Zero Point<br>$n \quad= $ Number of bits used for quantization</p><p>Note that in the derivation above we use unsigned integer to represent the quantized range. That is, $X_q∈[0,2^n−1]$. One could use signed integer if necessary (perhaps due to HW considerations). This can be achieved by subtracting $2^n−1$
.</p><h4 id=python-implementation class="relative group">Python Implementation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#python-implementation aria-label=Anchor>#</a></span></h4><p>Now let&rsquo;s implement Asymmetric quantization in python.</p><p>First, we import the required library. We are using pytorch here.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>_ <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>random<span style=color:#ff7b72;font-weight:700>.</span>manual_seed(<span style=color:#a5d6ff>1</span>)
</span></span></code></pre></div><p>Now, let&rsquo;s define the required functions:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>clamp</span>(X:torch<span style=color:#ff7b72;font-weight:700>.</span>Tensor, val_range:tuple):
</span></span><span style=display:flex><span>    <span style=color:#a5d6ff>&#34;&#34;&#34;Clamps the Tensor between given range&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    min_range, max_range <span style=color:#ff7b72;font-weight:700>=</span> val_range
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># X[X &lt; min_range] = min_range</span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># X[X&gt; max_range] = max_range</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>clip(X, min_range, max_range)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>scale</span>(X, target_bits):
</span></span><span style=display:flex><span>    X_max <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>max(X)
</span></span><span style=display:flex><span>    X_min <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>min(X)
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> (X_max <span style=color:#ff7b72;font-weight:700>-</span> X_min) <span style=color:#ff7b72;font-weight:700>/</span> (<span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>^</span>target_bits <span style=color:#ff7b72;font-weight:700>-</span> <span style=color:#a5d6ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>zero_point</span>(X, S):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> <span style=color:#ff7b72;font-weight:700>-</span> torch<span style=color:#ff7b72;font-weight:700>.</span>round(torch<span style=color:#ff7b72;font-weight:700>.</span>min(X)<span style=color:#ff7b72;font-weight:700>/</span> S)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>quantize</span>(X, target_bits):
</span></span><span style=display:flex><span>    S <span style=color:#ff7b72;font-weight:700>=</span> scale(X, target_bits)
</span></span><span style=display:flex><span>    Z <span style=color:#ff7b72;font-weight:700>=</span> zero_point(X, S)
</span></span><span style=display:flex><span>    X_q <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>round((X<span style=color:#ff7b72;font-weight:700>/</span>S) <span style=color:#ff7b72;font-weight:700>+</span> Z)
</span></span><span style=display:flex><span>    X_q <span style=color:#ff7b72;font-weight:700>=</span> clamp(X_q, (<span style=color:#a5d6ff>0</span>, <span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>^</span>target_bits <span style=color:#ff7b72;font-weight:700>-</span> <span style=color:#a5d6ff>1</span>)) 
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic>#  Range will be (-2^target_bits-1, 2^target_bits - 1) if we use signed integer</span>
</span></span><span style=display:flex><span>    torch_dype_map <span style=color:#ff7b72;font-weight:700>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#a5d6ff>8</span>: torch<span style=color:#ff7b72;font-weight:700>.</span>int8,
</span></span><span style=display:flex><span>        <span style=color:#a5d6ff>16</span>: torch<span style=color:#ff7b72;font-weight:700>.</span>int16,
</span></span><span style=display:flex><span>        <span style=color:#a5d6ff>32</span>: torch<span style=color:#ff7b72;font-weight:700>.</span>int32
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    dtype <span style=color:#ff7b72;font-weight:700>=</span> torch_dype_map[target_bits]
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> X_q<span style=color:#ff7b72;font-weight:700>.</span>to(dtype), S, Z
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>dequantize</span>(X_q, S, Z):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> S <span style=color:#ff7b72;font-weight:700>*</span> (X_q <span style=color:#ff7b72;font-weight:700>-</span> Z)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>quantization_error</span>(X, X_dequant):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>mean((X<span style=color:#ff7b72;font-weight:700>-</span> X_dequant)<span style=color:#ff7b72;font-weight:700>**</span><span style=color:#a5d6ff>2</span>)
</span></span></code></pre></div><p>Now, Let&rsquo;s See the results. We inititlilze a random tensor of size (5,5) and check the quantization error.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8b949e;font-style:italic># Let&#39;s inititalize a random tensor of size (5,5) between -127 and 127</span>
</span></span><span style=display:flex><span>X <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>FloatTensor(<span style=color:#a5d6ff>5</span>, <span style=color:#a5d6ff>5</span>)<span style=color:#ff7b72;font-weight:700>.</span>uniform_(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>127</span>, <span style=color:#a5d6ff>127</span>)
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;The original Tensor is: &#34;</span>)
</span></span><span style=display:flex><span>print(X)
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># Quantization</span>
</span></span><span style=display:flex><span>X_q, S, Z <span style=color:#ff7b72;font-weight:700>=</span> quantize(X, <span style=color:#a5d6ff>8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;After Quantization:&#34;</span>)
</span></span><span style=display:flex><span>print(X_q)
</span></span><span style=display:flex><span>print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Scale factor: </span><span style=color:#a5d6ff>{</span>S<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>, Zero Point: </span><span style=color:#a5d6ff>{</span>Z<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;After Dequantization: &#34;</span>)
</span></span><span style=display:flex><span>X_dequant <span style=color:#ff7b72;font-weight:700>=</span> dequantize(X_q, S, Z)
</span></span><span style=display:flex><span>print(X_dequant)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>q_mse <span style=color:#ff7b72;font-weight:700>=</span> quantization_error(X, X_dequant)
</span></span><span style=display:flex><span>print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Quantization Error: </span><span style=color:#a5d6ff>{</span>q_mse<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span></code></pre></div><p>OUTPUT:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>The original Tensor is: 
</span></span><span style=display:flex><span>tensor([[ -64.7480,   89.2118,    0.8170, -101.9451,  -26.6123],
</span></span><span style=display:flex><span>        [-114.6317,  -65.7990,  -89.9714,   86.8619,  -26.6812],
</span></span><span style=display:flex><span>        [ -21.9427,   60.8604,   51.6853,  -83.8159,   46.8039],
</span></span><span style=display:flex><span>        [  53.9525, -100.9789,  110.4185,  -77.2771,  -49.6635],
</span></span><span style=display:flex><span>        [-104.6081,  116.0752,   24.1830,  106.9384,    8.2932]])
</span></span><span style=display:flex><span>tensor([[1., 4., 2., -0., 1.],
</span></span><span style=display:flex><span>        [-0., 1., 0., 4., 1.],
</span></span><span style=display:flex><span>        [2., 3., 3., 0., 3.],
</span></span><span style=display:flex><span>        [3., -0., 4., 0., 1.],
</span></span><span style=display:flex><span>        [-0., 5., 3., 4., 2.]])
</span></span><span style=display:flex><span>After Quantization:
</span></span><span style=display:flex><span>tensor([[1, 4, 2, 0, 1],
</span></span><span style=display:flex><span>        [0, 1, 0, 4, 1],
</span></span><span style=display:flex><span>        [2, 3, 3, 0, 3],
</span></span><span style=display:flex><span>        [3, 0, 4, 0, 1],
</span></span><span style=display:flex><span>        [0, 5, 3, 4, 2]], dtype=torch.int8)
</span></span><span style=display:flex><span>Scale factor: 46.141387939453125, Zero Point: 2.0
</span></span><span style=display:flex><span>After Dequantization: 
</span></span><span style=display:flex><span>tensor([[-46.1414,  92.2828,   0.0000, -92.2828, -46.1414],
</span></span><span style=display:flex><span>        [-92.2828, -46.1414, -92.2828,  92.2828, -46.1414],
</span></span><span style=display:flex><span>        [  0.0000,  46.1414,  46.1414, -92.2828,  46.1414],
</span></span><span style=display:flex><span>        [ 46.1414, -92.2828,  92.2828, -92.2828, -46.1414],
</span></span><span style=display:flex><span>        [-92.2828, 138.4242,  46.1414,  92.2828,   0.0000]])
</span></span><span style=display:flex><span>Quantization Error: 202.06419372558594
</span></span></code></pre></div><p>As, we can see there is quite the data loss when quantizing the tensor with Mean Squared error 202.06, which is really high. But we have more optimized methods these days, so the data loss will be really small.
Also, see how the tensor performs when we dequantize it, we can see huge difference there also.</p><figure><img src=/img/quantization/symmetric_vs_aymmetric.png alt="Symmetric vs Asymmetric Quantization"><figcaption>Symmetric vs Asymmetric Quantization</figcaption></figure><h3 id=symmetric-quantization class="relative group">Symmetric Quantization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#symmetric-quantization aria-label=Anchor>#</a></span></h3><p>In symmetric quantization when converting from higher precision to lower precision, we can always restrict to values between $[-(2^{n-1} - 1), \space + 2^{n-1}-1]$ and ensure that the zero of the input perfectly maps to the zero of the output leading to a symmetric mapping.
For FLOAT16 to INT8 Conversion, we restrict the values between -127 to +127.</p><p>Here,
$$X_q = clamp \left( \frac{X}{S} ; - (2^{n-1} -1), (2^{n-1} - 1)\right)$$
Where,
$$ S = \frac{ |X|_{max}}{2^{n-1} - 1} $$</p><p>$$Z = 0$$</p><h4 id=python-implementation-1 class="relative group">Python Implementation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#python-implementation-1 aria-label=Anchor>#</a></span></h4><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch</span>
</span></span><span style=display:flex><span>_ <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>random<span style=color:#ff7b72;font-weight:700>.</span>manual_seed(<span style=color:#a5d6ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>clamp</span>(X:torch<span style=color:#ff7b72;font-weight:700>.</span>Tensor, val_range:tuple):
</span></span><span style=display:flex><span>    <span style=color:#a5d6ff>&#34;&#34;&#34;Clamps the Tensor between given range&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    min_range, max_range <span style=color:#ff7b72;font-weight:700>=</span> val_range
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># X[X &lt; min_range] = min_range</span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># X[X&gt; max_range] = max_range</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>clip(X, min_range, max_range)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>scale</span>(X, target_bits):
</span></span><span style=display:flex><span>    X_max <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>max(torch<span style=color:#ff7b72;font-weight:700>.</span>abs(X))
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> (X_max) <span style=color:#ff7b72;font-weight:700>/</span> (<span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>^</span>(target_bits<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>) <span style=color:#ff7b72;font-weight:700>-</span> <span style=color:#a5d6ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>quantize</span>(X, target_bits):
</span></span><span style=display:flex><span>    S <span style=color:#ff7b72;font-weight:700>=</span> scale(X, target_bits)
</span></span><span style=display:flex><span>    X_q <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>round(X<span style=color:#ff7b72;font-weight:700>/</span>S)
</span></span><span style=display:flex><span>    X_q <span style=color:#ff7b72;font-weight:700>=</span> clamp(X_q, (<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>^</span>(target_bits<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>) <span style=color:#ff7b72;font-weight:700>-</span> <span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>^</span>(target_bits<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>)<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>))
</span></span><span style=display:flex><span>    torch_dype_map <span style=color:#ff7b72;font-weight:700>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#a5d6ff>8</span>: torch<span style=color:#ff7b72;font-weight:700>.</span>int8,
</span></span><span style=display:flex><span>        <span style=color:#a5d6ff>16</span>: torch<span style=color:#ff7b72;font-weight:700>.</span>int16,
</span></span><span style=display:flex><span>        <span style=color:#a5d6ff>32</span>: torch<span style=color:#ff7b72;font-weight:700>.</span>int32
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    dtype <span style=color:#ff7b72;font-weight:700>=</span> torch_dype_map[target_bits]
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> X_q<span style=color:#ff7b72;font-weight:700>.</span>to(dtype), S
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>dequantize</span>(X_q, S):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> S <span style=color:#ff7b72;font-weight:700>*</span> X_q
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>quantization_error</span>(X, X_dequant):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>mean((X<span style=color:#ff7b72;font-weight:700>-</span> X_dequant)<span style=color:#ff7b72;font-weight:700>**</span><span style=color:#a5d6ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># Let&#39;s inititalize a random tensor of size (5,5) between -127 and 127</span>
</span></span><span style=display:flex><span>X <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>FloatTensor(<span style=color:#a5d6ff>5</span>, <span style=color:#a5d6ff>5</span>)<span style=color:#ff7b72;font-weight:700>.</span>uniform_(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>127</span>, <span style=color:#a5d6ff>127</span>)
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;The original Tensor is: &#34;</span>)
</span></span><span style=display:flex><span>print(X)
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># Quantization</span>
</span></span><span style=display:flex><span>X_q, S <span style=color:#ff7b72;font-weight:700>=</span> quantize(X, <span style=color:#a5d6ff>8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;After Quantization:&#34;</span>)
</span></span><span style=display:flex><span>print(X_q)
</span></span><span style=display:flex><span>print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Scale factor: </span><span style=color:#a5d6ff>{</span>S<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;After Dequantization: &#34;</span>)
</span></span><span style=display:flex><span>X_dequant <span style=color:#ff7b72;font-weight:700>=</span> dequantize(X_q, S)
</span></span><span style=display:flex><span>print(X_dequant)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>q_mse <span style=color:#ff7b72;font-weight:700>=</span> quantization_error(X, X_dequant)
</span></span><span style=display:flex><span>print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Quantization Error: </span><span style=color:#a5d6ff>{</span>q_mse<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span></code></pre></div><p>OUTPUT:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>The original Tensor is: 
</span></span><span style=display:flex><span>tensor([[   6.0810,   75.7082,   69.0292, -124.1490,   78.7299],
</span></span><span style=display:flex><span>        [  35.4792,  120.4665,   83.8276, -115.7145, -120.7527],
</span></span><span style=display:flex><span>        [ -61.2562,  111.5202,  -21.1543,   54.3508,  -59.0183],
</span></span><span style=display:flex><span>        [ 124.6147,  -53.7335,   95.2404,    1.5039,  -66.9058],
</span></span><span style=display:flex><span>        [  65.2799,  -67.4142,   37.3513,  -36.6722,  -13.9236]])
</span></span><span style=display:flex><span>After Quantization:
</span></span><span style=display:flex><span>tensor([[ 0,  2,  2, -4,  3],
</span></span><span style=display:flex><span>        [ 1,  4,  3, -4, -4],
</span></span><span style=display:flex><span>        [-2,  4, -1,  2, -2],
</span></span><span style=display:flex><span>        [ 4, -2,  3,  0, -2],
</span></span><span style=display:flex><span>        [ 2, -2,  1, -1,  0]], dtype=torch.int8)
</span></span><span style=display:flex><span>Scale factor: 31.153671264648438
</span></span><span style=display:flex><span>After Dequantization: 
</span></span><span style=display:flex><span>tensor([[   0.0000,   62.3073,   62.3073, -124.6147,   93.4610],
</span></span><span style=display:flex><span>        [  31.1537,  124.6147,   93.4610, -124.6147, -124.6147],
</span></span><span style=display:flex><span>        [ -62.3073,  124.6147,  -31.1537,   62.3073,  -62.3073],
</span></span><span style=display:flex><span>        [ 124.6147,  -62.3073,   93.4610,    0.0000,  -62.3073],
</span></span><span style=display:flex><span>        [  62.3073,  -62.3073,   31.1537,  -31.1537,    0.0000]])
</span></span><span style=display:flex><span>Quantization Error: 57.84915542602539
</span></span></code></pre></div><h4 id=but-which-method-is-better class="relative group">BUT, Which method is better? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#but-which-method-is-better aria-label=Anchor>#</a></span></h4><ul><li><p>Asymmetric range fully utilizes the quantized range because we exactly map the min-max values from float to the min-max range of quantized range.
While in Symmetric, if the float range is biased towards one side, it could result in a quantized range where significant dynamic range is dedicated to values that we&rsquo;ll never see. This could result in greater loss.</p></li><li><p>Also Zero point in asymmetric quantization leads extra weight on Hardware as it requires extra calculation, while the symmetric quantization is much simpler when we compare it to asymmetric. So we mostly use symmetric quantization.</p></li></ul><h4 id=choosing-scale-factor-and-zero-point class="relative group">Choosing Scale Factor and Zero point <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#choosing-scale-factor-and-zero-point aria-label=Anchor>#</a></span></h4><p>Above, we saw that the zero point of symmetric quantization is zero, while it is different in case of Asymmetric quantization. How do we decide this?
Let&rsquo;s take an example, Every integer or floating point number will have their own range (-128 to 127 for int8), the scaling factor essentially divides these numbers into equal factor. Since, when quantization, the high precision values should be reduced to lower precision, we need to clip those values at some point say alpha and beta for negative and positive values respectively. Any value beyond alpha and beta is not meaningful because it maps to the same output as that of alpha and beta. For the case of INT8 its -127 and +127 (we use -127 or numerical stability, this is called restricted quantization). The process of choosing these clipping values alpha and beta and hence the clipping range is called calibration.</p><p>To avoid cutting off too many numbers, a simple solution is to set alpha to $X_{\text{min}}$ and beta to $X_{\text{max}}$. Then we can easily figure out the scale factor, $S$, using these smallest and largest numbers. But this might make our counting uneven. For instance, if the largest number ($X_{\text{max}}$) is 1.5 and the smallest ($X_{\text{min}}$) is -1.2, our counting isn&rsquo;t balanced. To make it fair, we pick the larger number between the two ends and use that as our cutoff point on both sides. And we start counting from 0.</p><p>This balanced counting method is what we use when simplifying neural network weights. It&rsquo;s simpler because we always start counting from 0, making the math easier.</p><p>Now, let&rsquo;s consider when our numbers are mostly on one side, like the positive side. This is similar to the outputs of popular activation functions like ReLU or GeLU. Also, these activation outputs change based on the input. For example, showing the network two images of a cat might give different outputs. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>While, Minimum and maximum value works, sometimes we may see outliers that affect in quantization, in such cases, we can choose percentiles to choose the value of alpha and beta.</p><h2 id=modes-of-quantization class="relative group">Modes of Quantization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#modes-of-quantization aria-label=Anchor>#</a></span></h2><p>Based on when to quantize the model weights, quantization can be thought as two modes:</p><h3 id=1-post-training-quantization-ptq class="relative group">1. Post-Training Quantization (PTQ) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#1-post-training-quantization-ptq aria-label=Anchor>#</a></span></h3><p>In Post-Training quantization or PTQ, we quantize the weights of already trained model. This is straightforward and easy to implement, however it may degread the performance of model slihtly due to the loss of precision in the value of weights.<br>To better calibrate the model, model&rsquo;s weight and activations are evaluated on a representative dataset to determine the range of values (alpha, beta, scale, and zero-point) taken by these parameters. We then use these parameters to quantize the model.</p><p>Based on the methods of quantization, we can further divide PTQ into three categories:</p><ul><li><strong>Dynamic-Range Quantization</strong>: In this method, quantize the model based on the range of data globally. This method produces small model but there may be a bit more accuracy loss.</li><li><strong>Weight Quantization</strong>: In this method, we only quantize the weights of model leaving activations in their high precision. There may be higher accuracy loss with this method.</li><li><strong>Per-Channel Quantization</strong>: In this method, we quantize the model parameters based on the dynamic range per channel rather than globally. This helps in achieving optimal accuracy.</li></ul><h4 id=implementation-using-pytorch class="relative group">Implementation using PyTorch <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation-using-pytorch aria-label=Anchor>#</a></span></h4><p>Import all the necessary libraries</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>os</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>from</span> <span style=color:#ff7b72>tqdm</span> <span style=color:#ff7b72>import</span> tqdm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch.nn</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>nn</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torchvision.datasets</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>datasets</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torchvision.transforms</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>transforms</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>_ <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>manual_seed(<span style=color:#a5d6ff>0</span>)
</span></span></code></pre></div><p>Download and load the dataset:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transform <span style=color:#ff7b72;font-weight:700>=</span> transforms<span style=color:#ff7b72;font-weight:700>.</span>Compose([transforms<span style=color:#ff7b72;font-weight:700>.</span>ToTensor(), transforms<span style=color:#ff7b72;font-weight:700>.</span>Normalize((<span style=color:#a5d6ff>0.1301</span>, ), (<span style=color:#a5d6ff>0.3081</span>, ))])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_trainset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;./data&#34;</span>, train <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>True</span>, download<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_trainset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>, shuffle<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_testset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;./data&#34;</span>, train<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>False</span>, download<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>test_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_testset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>device(<span style=color:#a5d6ff>&#34;cpu&#34;</span>)
</span></span></code></pre></div><p>Create a simple pytorch model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>DigitsNet</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, input_shape, num_classes):
</span></span><span style=display:flex><span>        super(DigitsNet, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear1 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(input_shape, <span style=color:#a5d6ff>100</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear2 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>100</span>, <span style=color:#a5d6ff>100</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear3 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>100</span>,num_classes)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>relu <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>ReLU()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear1(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear3(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>relu(x)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> x
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>model <span style=color:#ff7b72;font-weight:700>=</span> DigitsNet(<span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>, <span style=color:#a5d6ff>10</span>)<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span></code></pre></div><p>Now, let&rsquo;s create a simple training loop to train and test the model</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>train</span>(train_loader, model, epochs <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>5</span>, limit<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>None</span>):
</span></span><span style=display:flex><span>    ce_loss <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>optim<span style=color:#ff7b72;font-weight:700>.</span>Adam(model<span style=color:#ff7b72;font-weight:700>.</span>parameters(), lr<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>0.001</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    total_iterations <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>for</span> epoch <span style=color:#ff7b72;font-weight:700>in</span> range(epochs):
</span></span><span style=display:flex><span>        model<span style=color:#ff7b72;font-weight:700>.</span>train()
</span></span><span style=display:flex><span>        loss_sum <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>        num_iterations <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>        data_iterator <span style=color:#ff7b72;font-weight:700>=</span> tqdm(train_loader, desc<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Epoch </span><span style=color:#a5d6ff>{</span>epoch<span style=color:#ff7b72;font-weight:700>+</span><span style=color:#a5d6ff>1</span><span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>if</span> limit <span style=color:#ff7b72;font-weight:700>is</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#79c0ff>None</span>:
</span></span><span style=display:flex><span>            data_iterator<span style=color:#ff7b72;font-weight:700>.</span>total <span style=color:#ff7b72;font-weight:700>=</span> limit
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> data <span style=color:#ff7b72;font-weight:700>in</span> data_iterator:
</span></span><span style=display:flex><span>            num_iterations <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>            total_iterations <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>            x, y <span style=color:#ff7b72;font-weight:700>=</span> data
</span></span><span style=display:flex><span>            x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            y <span style=color:#ff7b72;font-weight:700>=</span> y<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            optimizer<span style=color:#ff7b72;font-weight:700>.</span>zero_grad()
</span></span><span style=display:flex><span>            output <span style=color:#ff7b72;font-weight:700>=</span> model(x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>))
</span></span><span style=display:flex><span>            loss <span style=color:#ff7b72;font-weight:700>=</span> ce_loss(output, y)
</span></span><span style=display:flex><span>            loss_sum <span style=color:#ff7b72;font-weight:700>+=</span> loss
</span></span><span style=display:flex><span>            avg_loss <span style=color:#ff7b72;font-weight:700>=</span> loss_sum <span style=color:#ff7b72;font-weight:700>/</span> num_iterations
</span></span><span style=display:flex><span>            data_iterator<span style=color:#ff7b72;font-weight:700>.</span>set_postfix(loss<span style=color:#ff7b72;font-weight:700>=</span>avg_loss<span style=color:#ff7b72;font-weight:700>.</span>item())
</span></span><span style=display:flex><span>            loss<span style=color:#ff7b72;font-weight:700>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#ff7b72;font-weight:700>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>if</span> limit <span style=color:#ff7b72;font-weight:700>is</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#79c0ff>None</span> <span style=color:#ff7b72;font-weight:700>and</span> total_iterations <span style=color:#ff7b72;font-weight:700>&gt;=</span> limit:
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>test</span>(model):
</span></span><span style=display:flex><span>    correct <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    total <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    wrong_counts <span style=color:#ff7b72;font-weight:700>=</span> [<span style=color:#a5d6ff>0</span> <span style=color:#ff7b72>for</span> i <span style=color:#ff7b72;font-weight:700>in</span> range(<span style=color:#a5d6ff>10</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>with</span> torch<span style=color:#ff7b72;font-weight:700>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> data <span style=color:#ff7b72;font-weight:700>in</span> tqdm(test_loader, desc<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#34;Testing&#34;</span>):
</span></span><span style=display:flex><span>            x, y <span style=color:#ff7b72;font-weight:700>=</span> data
</span></span><span style=display:flex><span>            x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            y <span style=color:#ff7b72;font-weight:700>=</span> y<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            output<span style=color:#ff7b72;font-weight:700>=</span> model(x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>))
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>for</span> idx, i <span style=color:#ff7b72;font-weight:700>in</span> enumerate(output):
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>if</span> torch<span style=color:#ff7b72;font-weight:700>.</span>argmax(i) <span style=color:#ff7b72;font-weight:700>==</span> y[idx]:
</span></span><span style=display:flex><span>                    correct <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>else</span>:
</span></span><span style=display:flex><span>                    wrong_counts[y[idx]] <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>                total<span style=color:#ff7b72;font-weight:700>+=</span><span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>        print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Accuracy: </span><span style=color:#a5d6ff>{</span>round(correct<span style=color:#ff7b72;font-weight:700>/</span>total, <span style=color:#a5d6ff>3</span>) <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>100</span><span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>) 
</span></span></code></pre></div><p>Create a function to see the model size.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>print_model_size</span>(model):
</span></span><span style=display:flex><span>    torch<span style=color:#ff7b72;font-weight:700>.</span>save(model<span style=color:#ff7b72;font-weight:700>.</span>state_dict(), <span style=color:#a5d6ff>&#34;temp_model.pt&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#a5d6ff>&#34;Size (KB): &#34;</span>, os<span style=color:#ff7b72;font-weight:700>.</span>path<span style=color:#ff7b72;font-weight:700>.</span>getsize(<span style=color:#a5d6ff>&#34;temp_model.pt&#34;</span>)<span style=color:#ff7b72;font-weight:700>/</span><span style=color:#a5d6ff>1e3</span>)
</span></span><span style=display:flex><span>    os<span style=color:#ff7b72;font-weight:700>.</span>remove(<span style=color:#a5d6ff>&#34;temp_model.pt&#34;</span>)
</span></span></code></pre></div><p>Now, Let&rsquo;s train the model for 1 epochs.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train(train_loader, model, epochs<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Epoch 1: 100%|██████████| 6000/6000 [00:12&lt;00:00, 488.88it/s, loss=0.651]
</span></span></code></pre></div><p>Before quantizing the model, let&rsquo;s check the weight of a layer.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8b949e;font-style:italic># weights before quantization</span>
</span></span><span style=display:flex><span>print(model<span style=color:#ff7b72;font-weight:700>.</span>linear1<span style=color:#ff7b72;font-weight:700>.</span>weight)
</span></span><span style=display:flex><span>print(model<span style=color:#ff7b72;font-weight:700>.</span>linear1<span style=color:#ff7b72;font-weight:700>.</span>weight<span style=color:#ff7b72;font-weight:700>.</span>dtype)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Parameter containing:
</span></span><span style=display:flex><span>tensor([[-0.0267, -0.0073, -0.0558,  ..., -0.0045, -0.0227, -0.0243],
</span></span><span style=display:flex><span>        [-0.0445, -0.0397, -0.0352,  ..., -0.0450, -0.0307, -0.0547],
</span></span><span style=display:flex><span>        [-0.0326,  0.0025, -0.0457,  ..., -0.0328, -0.0113, -0.0044],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [ 0.0179,  0.0216, -0.0130,  ..., -0.0184,  0.0009, -0.0361],
</span></span><span style=display:flex><span>        [-0.0138, -0.0057,  0.0264,  ...,  0.0067,  0.0067,  0.0062],
</span></span><span style=display:flex><span>        [ 0.0057,  0.0003, -0.0138,  ...,  0.0226, -0.0267, -0.0065]],
</span></span><span style=display:flex><span>       requires_grad=True)
</span></span><span style=display:flex><span>torch.float32
</span></span></code></pre></div><p>As we can see our original weights are in <code>float32</code>.
Also, Let&rsquo;s check the size and accuracy of the model</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8b949e;font-style:italic># size of model</span>
</span></span><span style=display:flex><span>print_model_size(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># accuracy</span>
</span></span><span style=display:flex><span>test(model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Size (KB):  360.998
</span></span><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1746.29it/s]
</span></span><span style=display:flex><span>Accuracy: 90.60000000000001
</span></span></code></pre></div><p>Our original model size before quantizing is 360KB with 90.6% accuracy. Now let&rsquo;s quantize the model.</p><p>To quantize the model, we first create a exact copy of model with two extra layers, i.e. quant and dequant. These layers basically help us in finding the optimal range.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>QuantizedDigitsNet</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, input_shape, num_classes):
</span></span><span style=display:flex><span>        super(QuantizedDigitsNet, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>quant <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>QuantStub()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear1 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(input_shape, <span style=color:#a5d6ff>100</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear2 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>100</span>, <span style=color:#a5d6ff>100</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear3 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>100</span>,num_classes)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>relu <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>ReLU()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>dequant <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>DeQuantStub()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>quant(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear1(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear3(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>relu(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>dequant(x)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> x
</span></span></code></pre></div><p>As I said earlier, we introduce model with some test set to calibrate the range. This gives us idea about the values of $\alpha$, $\beta$, $S$ and $Z$.
In above model, we used two observers to observe the model behaviour. These observers calculate the required values when we calibrate the model. To calbrate it we simply pass the test set to dataset for a epoch.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># calibration</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch.ao.quantization</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_quantized <span style=color:#ff7b72;font-weight:700>=</span> QuantizedDigitsNet(<span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>, <span style=color:#a5d6ff>10</span>)<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_quantized<span style=color:#ff7b72;font-weight:700>.</span>load_state_dict(model<span style=color:#ff7b72;font-weight:700>.</span>state_dict())
</span></span><span style=display:flex><span>model_quantized<span style=color:#ff7b72;font-weight:700>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_quantized<span style=color:#ff7b72;font-weight:700>.</span>qconfig <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>ao<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>default_qconfig
</span></span><span style=display:flex><span>model_quantized <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>ao<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>prepare(model_quantized)
</span></span><span style=display:flex><span>model_quantized
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>QuantizedDigitsNet(
</span></span><span style=display:flex><span>  (quant): QuantStub(
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear1): Linear(
</span></span><span style=display:flex><span>    in_features=784, out_features=100, bias=True
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear2): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=100, bias=True
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear3): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=10, bias=True
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>  (dequant): DeQuantStub()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>As we can see above, we have MinMaxObserver, in each layer of our model. These observers when passed through test set calculate the of <code>min_val</code> and <code>max_val</code>.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>test(model_quantized)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1560.06it/s]
</span></span><span style=display:flex><span>Accuracy: 90.60000000000001
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_quantized
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>QuantizedDigitsNet(
</span></span><span style=display:flex><span>  (quant): QuantStub(
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-0.42226549983024597, max_val=2.8234341144561768)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear1): Linear(
</span></span><span style=display:flex><span>    in_features=784, out_features=100, bias=True
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-36.38924789428711, max_val=27.875974655151367)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear2): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=100, bias=True
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-41.35359191894531, max_val=32.791046142578125)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear3): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=10, bias=True
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-31.95071029663086, max_val=27.681312561035156)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>  (dequant): DeQuantStub()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Now we can see after calibration, we have min_val and max_val for each layer. Now using this, we quantize the model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#ff7b72;font-weight:700>.</span>backends<span style=color:#ff7b72;font-weight:700>.</span>quantized<span style=color:#ff7b72;font-weight:700>.</span>engine <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#39;qnnpack&#39;</span>
</span></span><span style=display:flex><span>model_quantized_net <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>ao<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>convert(model_quantized)
</span></span><span style=display:flex><span>model_quantized_net
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)
</span></span><span style=display:flex><span>  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.5060253739356995, zero_point=72, qscheme=torch.per_tensor_affine)
</span></span><span style=display:flex><span>  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.5838160514831543, zero_point=71, qscheme=torch.per_tensor_affine)
</span></span><span style=display:flex><span>  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.4695434868335724, zero_point=68, qscheme=torch.per_tensor_affine)
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>  (dequant): DeQuantize()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Now, after quantization, the model is keeping track of scale and zero point values. These values will be used while dequantizing.</p><p>Finally, let&rsquo;s compare the model weights before and after quantization and after dequantization</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;Before Quantization:</span><span style=color:#79c0ff>\n</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>print(model<span style=color:#ff7b72;font-weight:700>.</span>linear1<span style=color:#ff7b72;font-weight:700>.</span>weight)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;</span><span style=color:#79c0ff>\n</span><span style=color:#a5d6ff>After Quantization:</span><span style=color:#79c0ff>\n</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>print(torch<span style=color:#ff7b72;font-weight:700>.</span>int_repr(model_quantized_net<span style=color:#ff7b72;font-weight:700>.</span>linear1<span style=color:#ff7b72;font-weight:700>.</span>weight()))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;After Dequantization:</span><span style=color:#79c0ff>\n</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>print(torch<span style=color:#ff7b72;font-weight:700>.</span>dequantize(model_quantized_net<span style=color:#ff7b72;font-weight:700>.</span>linear1<span style=color:#ff7b72;font-weight:700>.</span>weight()))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Before Quantization:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Parameter containing:
</span></span><span style=display:flex><span>tensor([[-0.0267, -0.0073, -0.0558,  ..., -0.0045, -0.0227, -0.0243],
</span></span><span style=display:flex><span>        [-0.0445, -0.0397, -0.0352,  ..., -0.0450, -0.0307, -0.0547],
</span></span><span style=display:flex><span>        [-0.0326,  0.0025, -0.0457,  ..., -0.0328, -0.0113, -0.0044],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [ 0.0179,  0.0216, -0.0130,  ..., -0.0184,  0.0009, -0.0361],
</span></span><span style=display:flex><span>        [-0.0138, -0.0057,  0.0264,  ...,  0.0067,  0.0067,  0.0062],
</span></span><span style=display:flex><span>        [ 0.0057,  0.0003, -0.0138,  ...,  0.0226, -0.0267, -0.0065]],
</span></span><span style=display:flex><span>       requires_grad=True)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Before Quantization:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tensor([[ -9,  -3, -19,  ...,  -2,  -8,  -8],
</span></span><span style=display:flex><span>        [-15, -14, -12,  ..., -16, -11, -19],
</span></span><span style=display:flex><span>        [-11,   1, -16,  ..., -11,  -4,  -2],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [  6,   8,  -5,  ...,  -6,   0, -13],
</span></span><span style=display:flex><span>        [ -5,  -2,   9,  ...,   2,   2,   2],
</span></span><span style=display:flex><span>        [  2,   0,  -5,  ...,   8,  -9,  -2]], dtype=torch.int8)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>After Dequantization:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tensor([[-0.0259, -0.0086, -0.0546,  ..., -0.0057, -0.0230, -0.0230],
</span></span><span style=display:flex><span>        [-0.0431, -0.0402, -0.0345,  ..., -0.0460, -0.0316, -0.0546],
</span></span><span style=display:flex><span>        [-0.0316,  0.0029, -0.0460,  ..., -0.0316, -0.0115, -0.0057],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [ 0.0172,  0.0230, -0.0144,  ..., -0.0172,  0.0000, -0.0373],
</span></span><span style=display:flex><span>        [-0.0144, -0.0057,  0.0259,  ...,  0.0057,  0.0057,  0.0057],
</span></span><span style=display:flex><span>        [ 0.0057,  0.0000, -0.0144,  ...,  0.0230, -0.0259, -0.0057]])
</span></span></code></pre></div><p>As we can see, after quantization, the model weights are converted into <code>int8</code>.
Also, we can see the loss when dequantizing the quantized model. To check how much memory we gained and performance loss, let&rsquo;s check the accuracy and size of the model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print_model_size(model_quantized_net)
</span></span><span style=display:flex><span>test(model_quantized_net)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Size (KB):  95.266
</span></span><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1420.10it/s]
</span></span><span style=display:flex><span>Accuracy: 90.3
</span></span></code></pre></div><p>The size is reduced by almost 4 times. This is reasonable since we are reducing weights from <code>fp32</code> to <code>int8</code>. It is slightly more than 4 times, as we also need to store scale and other parameters after quantization.</p><p>Regarding the accuracy, we didn&rsquo;t loss that much from quantization. The loss is only 0.3%, which is really good.</p><p>While this is just a example, we may have more or less loss in real life models. There are many advance algorithms such as GPTQ, AWQ, GGUF to quantize the model after training. I will talk about them in later articles.</p><h3 id=2-quantization-aware-training-qat class="relative group">2. Quantization-Aware Training (QAT) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#2-quantization-aware-training-qat aria-label=Anchor>#</a></span></h3><p>Unlike PTQ, QAT integrates the weight conversion process during the training stage. This often results in superior model performance, but it&rsquo;s more computationally demanding. A highly used QAT technique is the QLoRA.
As we move to a lower precision from float, we generally notice a significant accuracy drop as this is a lossy process. This loss can be minimized with the help of quant-aware training. So basically, quant-aware training simulates low precision behavior in the forward pass,
while the backward pass remains the same. This induces some quantization error which is accumulated in the total loss of the model and hence the optimizer tries to reduce it by adjusting the parameters accordingly. This makes our parameters more robust to quantization making our process almost .</p><p>To introduce the quantization loss we introduce something known as FakeQuant nodes into our model after every operation involving computations to obtain the output in the range of our required precision. A FakeQuant node is basically a combination of Quantize and Dequantize operations stacked together.</p><h4 id=creating-qat-graph class="relative group">Creating QAT Graph <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#creating-qat-graph aria-label=Anchor>#</a></span></h4><p>Now that we have defined our FakeQuant nodes, we need to determine the correct position to insert them in the graph. We need to apply Quantize operations on our weights and activations using the following rules:</p><ul><li>Weights need to be quantized before they are multiplied or convolved with the input.</li><li>Our graph should display inference behavior while training so the BatchNorm layers must be folded and Dropouts must be removed.</li><li>Outputs of each layer are generally quantized after the activation layer like Relu is applied to them which is beneficial because most optimized hardware generally have the activation function fused with the main operation.</li><li>We also need to quantize the outputs of layers like Concat and Add where the outputs of several layers are merged.</li><li>We do not need to quantize the bias during training as we would be using int32 bias during inference and that can be calculated later on with the parameters obtained using the quantization of weights and activations.</li></ul><h4 id=training-qat-graph class="relative group">Training QAT Graph <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#training-qat-graph aria-label=Anchor>#</a></span></h4><p>Now that our graph is ready, we train the graph with quantization layers. While training we use the quantization layers only in forward pass to introduce extra quantization error which helps
is reducing quantization loss.</p><h4 id=inference-using-qat class="relative group">Inference using QAT <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#inference-using-qat aria-label=Anchor>#</a></span></h4><p>Now that our model is trained and ready, we take the quantized weights and quantize them using the parameters we get from QAT. Since, it only accepts quantized input, we also need to quantize the input while inferencing.</p><h4 id=pytorch-implementation class="relative group">PyTorch Implementation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pytorch-implementation aria-label=Anchor>#</a></span></h4><p>The functions to load data and train and testing loop are same for this operation. I will just go through the model creation and preparing model for training.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>os</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>from</span> <span style=color:#ff7b72>tqdm</span> <span style=color:#ff7b72>import</span> tqdm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch.nn</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>nn</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torchvision.datasets</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>datasets</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torchvision.transforms</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>transforms</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>_ <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>manual_seed(<span style=color:#a5d6ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transform <span style=color:#ff7b72;font-weight:700>=</span> transforms<span style=color:#ff7b72;font-weight:700>.</span>Compose([transforms<span style=color:#ff7b72;font-weight:700>.</span>ToTensor(), transforms<span style=color:#ff7b72;font-weight:700>.</span>Normalize((<span style=color:#a5d6ff>0.1301</span>, ), (<span style=color:#a5d6ff>0.3081</span>, ))])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_trainset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;./data&#34;</span>, train <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>True</span>, download<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_trainset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>, shuffle<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_testset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;./data&#34;</span>, train<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>False</span>, download<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>test_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_testset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>device(<span style=color:#a5d6ff>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>train</span>(train_loader, model, epochs <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>5</span>, limit<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>None</span>):
</span></span><span style=display:flex><span>    ce_loss <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>optim<span style=color:#ff7b72;font-weight:700>.</span>Adam(model<span style=color:#ff7b72;font-weight:700>.</span>parameters(), lr<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>0.001</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    total_iterations <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>for</span> epoch <span style=color:#ff7b72;font-weight:700>in</span> range(epochs):
</span></span><span style=display:flex><span>        model<span style=color:#ff7b72;font-weight:700>.</span>train()
</span></span><span style=display:flex><span>        loss_sum <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>        num_iterations <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>        data_iterator <span style=color:#ff7b72;font-weight:700>=</span> tqdm(train_loader, desc<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Epoch </span><span style=color:#a5d6ff>{</span>epoch<span style=color:#ff7b72;font-weight:700>+</span><span style=color:#a5d6ff>1</span><span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>if</span> limit <span style=color:#ff7b72;font-weight:700>is</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#79c0ff>None</span>:
</span></span><span style=display:flex><span>            data_iterator<span style=color:#ff7b72;font-weight:700>.</span>total <span style=color:#ff7b72;font-weight:700>=</span> limit
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> data <span style=color:#ff7b72;font-weight:700>in</span> data_iterator:
</span></span><span style=display:flex><span>            num_iterations <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>            total_iterations <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>            x, y <span style=color:#ff7b72;font-weight:700>=</span> data
</span></span><span style=display:flex><span>            x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            y <span style=color:#ff7b72;font-weight:700>=</span> y<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            optimizer<span style=color:#ff7b72;font-weight:700>.</span>zero_grad()
</span></span><span style=display:flex><span>            output <span style=color:#ff7b72;font-weight:700>=</span> model(x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>))
</span></span><span style=display:flex><span>            loss <span style=color:#ff7b72;font-weight:700>=</span> ce_loss(output, y)
</span></span><span style=display:flex><span>            loss_sum <span style=color:#ff7b72;font-weight:700>+=</span> loss
</span></span><span style=display:flex><span>            avg_loss <span style=color:#ff7b72;font-weight:700>=</span> loss_sum <span style=color:#ff7b72;font-weight:700>/</span> num_iterations
</span></span><span style=display:flex><span>            data_iterator<span style=color:#ff7b72;font-weight:700>.</span>set_postfix(loss<span style=color:#ff7b72;font-weight:700>=</span>avg_loss<span style=color:#ff7b72;font-weight:700>.</span>item())
</span></span><span style=display:flex><span>            loss<span style=color:#ff7b72;font-weight:700>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#ff7b72;font-weight:700>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>if</span> limit <span style=color:#ff7b72;font-weight:700>is</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#79c0ff>None</span> <span style=color:#ff7b72;font-weight:700>and</span> total_iterations <span style=color:#ff7b72;font-weight:700>&gt;=</span> limit:
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>return</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>print_model_size</span>(model):
</span></span><span style=display:flex><span>    torch<span style=color:#ff7b72;font-weight:700>.</span>save(model<span style=color:#ff7b72;font-weight:700>.</span>state_dict(), <span style=color:#a5d6ff>&#34;temp_model.pt&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#a5d6ff>&#34;Size (KB): &#34;</span>, os<span style=color:#ff7b72;font-weight:700>.</span>path<span style=color:#ff7b72;font-weight:700>.</span>getsize(<span style=color:#a5d6ff>&#34;temp_model.pt&#34;</span>)<span style=color:#ff7b72;font-weight:700>/</span><span style=color:#a5d6ff>1e3</span>)
</span></span><span style=display:flex><span>    os<span style=color:#ff7b72;font-weight:700>.</span>remove(<span style=color:#a5d6ff>&#34;temp_model.pt&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>test</span>(model):
</span></span><span style=display:flex><span>    correct <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    total <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    wrong_counts <span style=color:#ff7b72;font-weight:700>=</span> [<span style=color:#a5d6ff>0</span> <span style=color:#ff7b72>for</span> i <span style=color:#ff7b72;font-weight:700>in</span> range(<span style=color:#a5d6ff>10</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>with</span> torch<span style=color:#ff7b72;font-weight:700>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> data <span style=color:#ff7b72;font-weight:700>in</span> tqdm(test_loader, desc<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#34;Testing&#34;</span>):
</span></span><span style=display:flex><span>            x, y <span style=color:#ff7b72;font-weight:700>=</span> data
</span></span><span style=display:flex><span>            x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            y <span style=color:#ff7b72;font-weight:700>=</span> y<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            output<span style=color:#ff7b72;font-weight:700>=</span> model(x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>))
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>for</span> idx, i <span style=color:#ff7b72;font-weight:700>in</span> enumerate(output):
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>if</span> torch<span style=color:#ff7b72;font-weight:700>.</span>argmax(i) <span style=color:#ff7b72;font-weight:700>==</span> y[idx]:
</span></span><span style=display:flex><span>                    correct <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>else</span>:
</span></span><span style=display:flex><span>                    wrong_counts[y[idx]] <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>                total<span style=color:#ff7b72;font-weight:700>+=</span><span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>        print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Accuracy: </span><span style=color:#a5d6ff>{</span>round(correct<span style=color:#ff7b72;font-weight:700>/</span>total, <span style=color:#a5d6ff>3</span>) <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>100</span><span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span></code></pre></div><p>Let&rsquo;s create a model. Previously, we created a model and trained without quantization. But in this method, we introduce fake quantization layer before training. So, we add quant and dequant stub in the model creation process like below:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>DigitsNet</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, input_shape, num_classes):
</span></span><span style=display:flex><span>        super(DigitsNet, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>quant  <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>QuantStub()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear1 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(input_shape, <span style=color:#a5d6ff>100</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear2 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>100</span>, <span style=color:#a5d6ff>100</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear3 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>100</span>,num_classes)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>relu <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>ReLU()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>dequant <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>DeQuantStub()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>quant(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear1(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear3(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>relu(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>dequant(x)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> x
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>model <span style=color:#ff7b72;font-weight:700>=</span> DigitsNet(<span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>, <span style=color:#a5d6ff>10</span>)<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span></code></pre></div><p>Now, we also introduce observers which helps us in getting the quantization range and other parameters.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#ff7b72;font-weight:700>.</span>qconfig <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>ao<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>default_qconfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#ff7b72;font-weight:700>.</span>train()
</span></span><span style=display:flex><span>model_quantized <span style=color:#ff7b72;font-weight:700>=</span>  torch<span style=color:#ff7b72;font-weight:700>.</span>ao<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>prepare_qat(model)
</span></span><span style=display:flex><span>model_quantized
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>DigitsNet(
</span></span><span style=display:flex><span>  (quant): QuantStub(
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear1): Linear(
</span></span><span style=display:flex><span>    in_features=784, out_features=100, bias=True
</span></span><span style=display:flex><span>    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear2): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=100, bias=True
</span></span><span style=display:flex><span>    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear3): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=10, bias=True
</span></span><span style=display:flex><span>    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>  (dequant): DeQuantStub()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>As you can see, we have introduced fake quantization layers in between each layer.
Now let&rsquo;s train the model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train(train_loader, model_quantized, epochs<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Epoch 1: 100%|██████████| 6000/6000 [00:12&lt;00:00, 466.85it/s, loss=0.419]
</span></span></code></pre></div><p>After training, observers collect the range information.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(model_quantized)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>  (quant): QuantStub(
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-0.42226549983024597, max_val=2.8234341144561768)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear1): Linear(
</span></span><span style=display:flex><span>    in_features=784, out_features=100, bias=True
</span></span><span style=display:flex><span>    (weight_fake_quant): MinMaxObserver(min_val=-0.3858276903629303, max_val=0.4076198637485504)
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-23.91692352294922, max_val=28.753376007080078)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear2): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=100, bias=True
</span></span><span style=display:flex><span>    (weight_fake_quant): MinMaxObserver(min_val=-0.25747808814048767, max_val=0.2415604591369629)
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-38.25144577026367, max_val=34.10194778442383)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear3): Linear(
</span></span><span style=display:flex><span>    in_features=100, out_features=10, bias=True
</span></span><span style=display:flex><span>    (weight_fake_quant): MinMaxObserver(min_val=-0.16331200301647186, max_val=0.1910468488931656)
</span></span><span style=display:flex><span>    (activation_post_process): MinMaxObserver(min_val=-33.441688537597656, max_val=37.548683166503906)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>  (dequant): DeQuantStub()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>As, we have the required data, we quantize the model and perform inference using that model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#ff7b72;font-weight:700>.</span>backends<span style=color:#ff7b72;font-weight:700>.</span>quantized<span style=color:#ff7b72;font-weight:700>.</span>engine <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#39;qnnpack&#39;</span>
</span></span><span style=display:flex><span>model_quantized<span style=color:#ff7b72;font-weight:700>.</span>eval()
</span></span><span style=display:flex><span>model_quantized <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>ao<span style=color:#ff7b72;font-weight:700>.</span>quantization<span style=color:#ff7b72;font-weight:700>.</span>convert(model_quantized)
</span></span><span style=display:flex><span>print(model_quantized)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>DigitsNet(
</span></span><span style=display:flex><span>  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)
</span></span><span style=display:flex><span>  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.41472676396369934, zero_point=58, qscheme=torch.per_tensor_affine)
</span></span><span style=display:flex><span>  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.5697117447853088, zero_point=67, qscheme=torch.per_tensor_affine)
</span></span><span style=display:flex><span>  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.558979332447052, zero_point=60, qscheme=torch.per_tensor_affine)
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>  (dequant): DeQuantize()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>We can see here the model is quantized. Let&rsquo;s check the model size before and after quantization</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print_model_size(model)
</span></span><span style=display:flex><span>print_model_size(model_quantized)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Size (KB):  361.062
</span></span><span style=display:flex><span>Size (KB):  95.266
</span></span></code></pre></div><p>As with the PTQ, the model size is decreased by almost 4 times.
Let&rsquo;s test the model accuracy and see the quantized weights.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>test(model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1611.47it/s]
</span></span><span style=display:flex><span>Accuracy: 89.9
</span></span></code></pre></div><p>The model is performing a bit badly than PTQ, but that&rsquo;s not the case with all the model. This would be better in most cases.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(torch<span style=color:#ff7b72;font-weight:700>.</span>int_repr(model_quantized<span style=color:#ff7b72;font-weight:700>.</span>linear1<span style=color:#ff7b72;font-weight:700>.</span>weight()))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>tensor([[  0,  -2,  -2,  ...,  -1, -11,   1],
</span></span><span style=display:flex><span>        [  1,  -6,   5,  ...,  -1,  -1,  10],
</span></span><span style=display:flex><span>        [  1,  -8,  -5,  ...,  -9,   3,  -3],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [  7,   3,  -7,  ...,   6,  12,  -5],
</span></span><span style=display:flex><span>        [  2,  -9, -16,  ..., -12, -13,  -2],
</span></span><span style=display:flex><span>        [ -3, -13,  -3,  ..., -10,   2,   1]], dtype=torch.int8)
</span></span></code></pre></div><p>As, we can see the model is indeed quantized into int8.</p><h3 id=conclusion class="relative group">Conclusion <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusion aria-label=Anchor>#</a></span></h3><p>In conclusion, quantization emerges as a pivotal solution to mitigate the computational barriers posed by Large Language Models (LLMs). While these models offer unparalleled capabilities in various tasks, their size necessitates significant resources for efficient execution. Quantization addresses this challenge by converting model parameters from higher precision formats like FLOAT32 to lower precision formats such as INT8 or INT4, substantially reducing storage and memory requirements. Whether through asymmetric or symmetric methods, quantization offers a trade-off between computational efficiency and accuracy preservation, enabling the execution of LLMs on standard consumer-grade hardware.</p><p>Moreover, modes of quantization, such as Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), provide flexibility in integrating quantization into the model lifecycle. PTQ simplifies the process by quantizing pre-trained models, albeit with potential accuracy loss, while QAT, although computationally demanding, offers superior performance by integrating quantization into the training stage. As research and development in quantization techniques progress, we anticipate further advancements in efficiency and performance, ultimately democratizing access to advanced AI capabilities and fostering widespread adoption across diverse hardware infrastructures.</p><h2 id=recommended-readings class="relative group">Recommended Readings <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#recommended-readings aria-label=Anchor>#</a></span></h2><ol><li><a href=https://intellabs.github.io/distiller/algo_quantization.html target=_blank rel=noreferrer>Quantization Algorithms</a></li><li><a href=https://towardsdatascience.com/tensor-quantization-the-untold-story-d798c30e7646 target=_blank rel=noreferrer>Tensor Quantization: The Untold Story</a></li><li><a href=https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c target=_blank rel=noreferrer>Introduction to Weight Quantization</a></li><li><a href=https://towardsdatascience.com/inside-quantization-aware-training-4f91c8837ead target=_blank rel=noreferrer>Inside Quantization Aware Training56</a></li><li><a href=https:/medium.com/@jan_marcel_kezmann/master-the-art-of-quantization-a-practical-guide-e74d7aad24f9 target=_blank rel=noreferrer>Master the Art of Quantization: A Practical Guide</a></li><li><a href="https://www.youtube.com/watch?v=0VdNflU08yA" target=_blank rel=noreferrer>Quantization explained with PyTorch - Post-Training Quantization, Quantization-Aware Training</a></li></ol><h2 id=references class="relative group">References <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#references aria-label=Anchor>#</a></span></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.ai-bites.net/model-quantization-in-deep-learning/ target=_blank rel=noreferrer>Model Quantization in Deep Learning</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div></article></div><footer><div class="mx-auto max-w-3xl px-4 flex justify-between py-8 md:pt-40 md:pb-20"><a href=/ aria-label=Home><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 79 79"><g transform="translate(-502 -524)"><rect width="79" height="79" rx="14" transform="translate(502 524)" fill="#14b8a6"/><text transform="translate(521 587)" fill="#0f172a" font-size="63" font-family="Geist" font-weight="700"><tspan x="0" y="0">B</tspan></text></g></svg></a><div class=text-right><nav class="text-sm md:text-base"><ul class="flex flex-col gap-4 md:gap-8 sm:flex-row justify-end"><li><a href=https://github.com/timilsinabimal/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Github</a></li><li><a href=https://linkedin.com/in/TimilsinaBimal/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>LinkedIn</a></li><li><a href=https://instagram.com/__bimal_/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Instagram</a></li><li><a href=https://twitter.com/TimilsinaBml/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Twitter</a></li></ul></nav><span class="text-gray-500 mt-4 block sm:mt-2 text-sm">&copy;2024 Bimal Timilisna</span></div></div></footer><button id=to-top-button onclick=scrollToTop() title="Go To Top" class="hidden fixed z-50 bottom-10 right-10 lg:bottom:20 lg:right:20 sm:bottom:5 sm:right:5 md:bottom:5 md:right:5 p-3 border-0 w-11 h-11 rounded-full shadow-md bg-teal-300 hover:bg-teal-500 text-white text-lg font-semibold transition-colors duration-300">
<svg clip-rule="evenodd" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m18.787 9.473s-4.505-4.502-6.259-6.255c-.147-.146-.339-.22-.53-.22-.192.0-.384.074-.531.22C9.714 4.971 5.211 9.47 5.211 9.47c-.147.147-.219.339-.217.532.001.19.075.38.221.525.292.293.766.295 1.056.004l4.977-4.976v14.692c0 .414.336.75.75.75.413.0.75-.336.75-.75V5.555l4.978 4.978c.289.29.762.287 1.055-.006.145-.145.219-.335.221-.525.002-.192-.07-.384-.215-.529z" fill-rule="nonzero"/></svg>
<span class=sr-only>Go to top</span></button></main></div></body></html>