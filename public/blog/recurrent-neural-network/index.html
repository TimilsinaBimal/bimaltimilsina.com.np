<!doctype html><html lang=en class=scroll-smooth><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Understanding Recurrent Neural Networks (RNNs) for Sequential Data Processing | Bimal Timilsina</title>
<meta property="og:title" content="Understanding Recurrent Neural Networks (RNNs) for Sequential Data Processing | Bimal Timilsina"><meta property="og:description" content="Recurrent Neural Networks (RNNs) are specialized neural networks capable of processing sequential data by retaining memory of previous inputs through hidden states, facilitating context-aware predictions. Despite their effectiveness with sequential data, RNNs face challenges such as vanishing gradients, diminishing the impact of distant inputs, and exploding gradients, causing numerical instability, which can be alleviated through techniques like gradient clipping."><link rel=icon type=image/svg href=http://bimaltimilsina.com.np/favicon.svg hreflang=en-us><meta property="og:type" content="website"><meta property="og:url" content="http://bimaltimilsina.com.np/blog/recurrent-neural-network/"><meta property="og:site_name" content="Understanding Recurrent Neural Networks (RNNs) for Sequential Data Processing | Bimal Timilsina"><meta property="og:image" content><meta property="og:image:width" content="1280"><meta property="og:image:height" content="800"><meta property="og:image:type" content="image/png"><meta property="og:locale" content="en_US"><meta name=description content="Recurrent Neural Networks (RNNs) are specialized neural networks capable of processing sequential data by retaining memory of previous inputs through hidden states, facilitating context-aware predictions. Despite their effectiveness with sequential data, RNNs face challenges such as vanishing gradients, diminishing the impact of distant inputs, and exploding gradients, causing numerical instability, which can be alleviated through techniques like gradient clipping."><meta name=keywords content="development machine-learning bimal bimal-timilsina timilsina llm large-language-models rag"><meta itemprop=name content="Understanding Recurrent Neural Networks (RNNs) for Sequential Data Processing | Bimal Timilsina"><meta itemprop=description content="Recurrent Neural Networks (RNNs) are specialized neural networks capable of processing sequential data by retaining memory of previous inputs through hidden states, facilitating context-aware predictions. Despite their effectiveness with sequential data, RNNs face challenges such as vanishing gradients, diminishing the impact of distant inputs, and exploding gradients, causing numerical instability, which can be alleviated through techniques like gradient clipping."><meta itemprop=image content><meta name=twitter:card content="summary"><meta name=twitter:url content="http://bimaltimilsina.com.np/blog/recurrent-neural-network/"><meta name=twitter:site content="@timilsinabml05"><meta name=twitter:creator content="@timilsinabml05"><meta name=twitter:title content="Understanding Recurrent Neural Networks (RNNs) for Sequential Data Processing | Bimal Timilsina"><meta name=twitter:description content="Recurrent Neural Networks (RNNs) are specialized neural networks capable of processing sequential data by retaining memory of previous inputs through hidden states, facilitating context-aware predictions. Despite their effectiveness with sequential data, RNNs face challenges such as vanishing gradients, diminishing the impact of distant inputs, and exploding gradients, causing numerical instability, which can be alleviated through techniques like gradient clipping."><meta name=twitter:image:src content><link rel=canonical href=http://bimaltimilsina.com.np/blog/recurrent-neural-network/><link rel=robots href=/robots.txt><meta name=msapplication-TileColor content="#0f172a"><meta name=theme-color content="#0f172a"><meta name=next-head-count content="29"><script src=https://cdn.tailwindcss.com></script><link rel=stylesheet href=http://bimaltimilsina.com.np/css/style.css><script src=https://cdn.jsdelivr.net/npm/js-polyfills@0.1.43/polyfill.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js type=text/javascript></script><script src=https://unpkg.com/scrollreveal/dist/scrollreveal.min.js></script><script src=http://bimaltimilsina.com.np/js/site_main.js defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script></head><body class="bg-slate-900 leading-relaxed text-slate-400 selection:bg-teal-300 selection:text-teal-900"><div class="font-sans group/spotlight relative"><div id=glow_box class="pointer-events-none position-fixed inset-0 z-30 transition duration-300 lg:absolute"></div><main><div class="mx-auto max-w-3xl px-4"><article><div><header class="pt-16 md:pt-20 lg:pt-28 mb-10 md:mb-20"><a class="group mb-2 inline-flex items-center font-semibold leading-tight text-teal-300" href=http://bimaltimilsina.com.np/blog/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor" class="mr-1 h-4 w-4 rotate-180 transition-transform group-hover:-translate-x-2" aria-hidden="true"><path fill-rule="evenodd" d="M3 10a.75.75.0 01.75-.75h10.638L10.23 5.29a.75.75.0 111.04-1.08l5.5 5.25a.75.75.0 010 1.08l-5.5 5.25a.75.75.0 11-1.04-1.08l4.158-3.96H3.75A.75.75.0 013 10z" clip-rule="evenodd"/></svg>Blog</a><h1 class="text-3xl font-semibold tracking-tight leading-tight text-gray-900 md:text-4xl lg:text-4xl dark:text-white">Understanding Recurrent Neural Networks (RNNs) for Sequential Data Processing</h1><div class="text-gray-400 mt-6"><time>Apr 11, 2024</time> — <span>4
mins read</span> — <a class="group mb-2 inline-flex items-center font-semibold leading-tight text-teal-300" href=http://bimaltimilsina.com.np/categories/nlp>NLP</a></div></header><div class="prose md:prose-lg dark:prose-invert article prose-headings:font-semibold mt-6"><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>You may have already worked with feedforward neural networks, these neural networks take input and after some computation in hidden layers they produce output. which works in most of the cases but when it comes with sequential data like text, sound, stock price we cannot just predict next value from only one previous inputs, we need some contexts to generate the next output which is impossible with feedforward neural networks.</p><h3 id=what-is-sequential-data class="relative group">What is sequential data? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#what-is-sequential-data aria-label=Anchor>#</a></span></h3><p>Sequential data is that type of data that comes in sequence. Consider we have a weather data. The weather of the next day depends not only on other factors but also on previous days’ weather too. So, the data that depends on previous data for future prediction is called sequential data. Text, Weather data, stock prices, audio data are the best example of sequential data.</p><p>So, let’s head back to RNNs, RNNs are that type of neural networks that can process these sequential data. They use memory cells to keep track of previous inputs in the form of hidden state and use it to compute the overall prediction of neural network.</p><p><figure><img src=/img/rnn/architecture-rnn-ltr.png alt=Bimal class="mx-auto my-0 rounded-md"></figure></p><p>Here, we can see each RNN units returns two outputs, one hidden state that is passed to another RNN cell for next prediction and another prediction of that cell.</p><p>Here,</p><p>$x^{&lt;t>}$ = input data of $t^{th}$ timestep</p><p>$a^{&lt;t>}$ = hidden state of $t^{th}$ timestep</p><p>$y^{&lt;t>}$ = output of $t^{th}$ timestep</p><p>Basically, the first input takes initial hidden state $a^{&lt;0>}$ which we initialize randomly along with input $x^{&lt;1>}$ which produces the output $y^{<t>}$ using the following formula.</p><p>$$
a^{&lt;1>} = g(W_{aa}a^{0} + W_{ax}X^{1} + b_a)
$$</p><p>$$
\hat{y}^{&lt;1>} = g&rsquo;(W_{ya}a^{&lt;1>} + b_y)
$$</p><p>In general,</p><p>$$
a^{&lt;t>} = g(W_{aa}a^{t-1} + W_{ax}X^{t} + b_a)
$$</p><p>$$
\hat{y}^{&lt;t>} = g&rsquo;(W_{ya}a^{&lt;t>} + b_y)
$$</p><p>Where,</p><p>$W_{aa}$ is the weight vector of hidden state</p><p>$W_{ax}$ is the weight vector of input edge</p><p>$W_{ya}$ is the weight vector of output</p><p>$b_a$ is the bias term for hidden state</p><p>$b_y$ is the bias term for output</p><p>In each time-step, we use previous hidden state and current input to calculate the next output. Since RNNs share parameters across time, here we are using only two sets of weight vector for every time steps mainly for input state and hidden state. The output layer is just as same as the feedforward neural network.</p><p>The back-propagation operation is same as we do in feedforward neural networks but since we are using different time-steps, we average the gradients of every timestep to calculate the final output, and the we call this backpropagation, <em>backpropagation through time.</em></p><p><figure><img src=https://editor.analyticsvidhya.com/uploads/86855simplilearn.gif alt=https://editor.analyticsvidhya.com/uploads/86855simplilearn.gif class="mx-auto my-0 rounded-md"></figure></p><h2 id=types-of-recurrent-neural-networks class="relative group">Types of Recurrent Neural Networks <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#types-of-recurrent-neural-networks aria-label=Anchor>#</a></span></h2><ol><li><p>One to One</p><p>In this type of RNNs there will be one input and one output.</p><p>example:</p><ul><li>Traditional Neural Networks</li></ul><p><figure><img src=/img/rnn/one-to-one.webp alt=one-to-one.webp class="mx-auto my-0 rounded-md"></figure></p></li><li><p>One to Many</p><p>If there is only one input and many outputs then this type of RNN is called One to Many RNN. example:</p><ul><li>generating music from a single tune or empty subset.</li></ul><p><figure><img src=/img/rnn/one-to-many.png alt=one-to-many.png class="mx-auto my-0 rounded-md"></figure></p></li><li><p>Many to One</p><p>When there are multiple inputs but the output is only one, then this type of RNN is called many to one RNN.</p><p>example:</p><ul><li>Calculating sentiment from long text.</li><li>Classifying news articles to different categories.</li></ul><p><figure><img src=/img/rnn/many-to-one.webp alt=many-to-one.webp class="mx-auto my-0 rounded-md"></figure></p></li><li><p>Many to Many</p><p>If there are many inputs and many outputs then this type of RNN is called Many to Many architecture.</p><p>example:</p><ul><li>Text generation</li><li>Language Translation</li></ul><p><figure><img src=/img/rnn/many-to-many.png alt=many-to-many.png class="mx-auto my-0 rounded-md"></figure></p></li></ol><h2 id=cons-of-rnns class="relative group">Cons of RNNs <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#cons-of-rnns aria-label=Anchor>#</a></span></h2><h3 id=vanishing-gradients class="relative group">Vanishing Gradients <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vanishing-gradients aria-label=Anchor>#</a></span></h3><p>While RNNs works best with sequential data, they struggle with large sequences. If we have long sequence of text and we need to generate text based on that, we need to have context of the words present in beginning of sentence to correctly predict the word in the next sequence, but since it cannot hold a lot of information, there will be almost no impact of these words in the sequence. This type of problem is called vanishing gradients.</p><p>Mathematically, If the sequence is too large and the weights becomes less than 1 then the impact of words which are far before current words will be low while predicting current word and the prediction may become wrong.</p><h3 id=exploding-gradients class="relative group">Exploding Gradients <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#exploding-gradients aria-label=Anchor>#</a></span></h3><p>If the weights of RNN > 1 then the weights may increase exponentially in every iteration and can cause numerical overflow.</p><p>We can reduce exploding gradients problem using gradient clipping.</p></div></div></article></div><footer><div class="mx-auto max-w-3xl px-4 flex justify-between py-8 md:pt-40 md:pb-20"><a href=/ aria-label=Home><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 79 79"><g transform="translate(-502 -524)"><rect width="79" height="79" rx="14" transform="translate(502 524)" fill="#14b8a6"/><text transform="translate(521 587)" fill="#0f172a" font-size="63" font-family="Geist" font-weight="700"><tspan x="0" y="0">B</tspan></text></g></svg></a><div class=text-right><nav class="text-sm md:text-base"><ul class="flex flex-col gap-4 md:gap-8 sm:flex-row justify-end"><li><a href=https://github.com/timilsinabimal/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Github</a></li><li><a href=https://linkedin.com/in/TimilsinaBimal/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>LinkedIn</a></li><li><a href=https://instagram.com/__bimal_/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Instagram</a></li><li><a href=https://twitter.com/TimilsinaBml/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Twitter</a></li></ul></nav><span class="text-gray-500 mt-4 block sm:mt-2 text-sm">&copy;2024 Bimal Timilisna</span></div></div></footer><button id=to-top-button onclick=scrollToTop() title="Go To Top" class="hidden fixed z-50 bottom-10 right-10 lg:bottom:20 lg:right:20 sm:bottom:5 sm:right:5 md:bottom:5 md:right:5 p-3 border-0 w-11 h-11 rounded-full shadow-md bg-teal-300 hover:bg-teal-500 text-white text-lg font-semibold transition-colors duration-300">
<svg clip-rule="evenodd" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m18.787 9.473s-4.505-4.502-6.259-6.255c-.147-.146-.339-.22-.53-.22-.192.0-.384.074-.531.22C9.714 4.971 5.211 9.47 5.211 9.47c-.147.147-.219.339-.217.532.001.19.075.38.221.525.292.293.766.295 1.056.004l4.977-4.976v14.692c0 .414.336.75.75.75.413.0.75-.336.75-.75V5.555l4.978 4.978c.289.29.762.287 1.055-.006.145-.145.219-.335.221-.525.002-.192-.07-.384-.215-.529z" fill-rule="nonzero"/></svg>
<span class=sr-only>Go to top</span></button></main></div></body></html>