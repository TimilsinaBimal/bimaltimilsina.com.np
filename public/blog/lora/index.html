<!doctype html><html lang=en class=scroll-smooth><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Revolutionizing LLM Fine-Tuning:Low-Rank Adaptation (LoRA) | Bimal Timilsina</title>
<meta property="og:title" content="Revolutionizing LLM Fine-Tuning:Low-Rank Adaptation (LoRA) | Bimal Timilsina"><meta property="og:description" content="LoRA is reshaping how we fine-tune neural networks, offering lightning-fast training, reduced memory footprint, and impressive performance gains. With LoRA, achieving optimal model performance is not just efficient—it's a game-changer for AI development."><link rel=icon type=image/svg href=http://bimaltimilsina.com.np/favicon.svg hreflang=en-us><meta property="og:type" content="website"><meta property="og:url" content="http://bimaltimilsina.com.np/blog/lora/"><meta property="og:site_name" content="Revolutionizing LLM Fine-Tuning:Low-Rank Adaptation (LoRA) | Bimal Timilsina"><meta property="og:image" content="lora.jpg"><meta property="og:image:width" content="1280"><meta property="og:image:height" content="800"><meta property="og:image:type" content="image/png"><meta property="og:locale" content="en_US"><meta name=description content="LoRA is reshaping how we fine-tune neural networks, offering lightning-fast training, reduced memory footprint, and impressive performance gains. With LoRA, achieving optimal model performance is not just efficient—it's a game-changer for AI development."><meta name=keywords content="development machine-learning bimal bimal-timilsina timilsina llm large-language-models rag"><meta itemprop=name content="Revolutionizing LLM Fine-Tuning:Low-Rank Adaptation (LoRA) | Bimal Timilsina"><meta itemprop=description content="LoRA is reshaping how we fine-tune neural networks, offering lightning-fast training, reduced memory footprint, and impressive performance gains. With LoRA, achieving optimal model performance is not just efficient—it's a game-changer for AI development."><meta itemprop=image content="lora.jpg"><meta name=twitter:card content="summary"><meta name=twitter:url content="http://bimaltimilsina.com.np/blog/lora/"><meta name=twitter:site content="@timilsinabml05"><meta name=twitter:creator content="@timilsinabml05"><meta name=twitter:title content="Revolutionizing LLM Fine-Tuning:Low-Rank Adaptation (LoRA) | Bimal Timilsina"><meta name=twitter:description content="LoRA is reshaping how we fine-tune neural networks, offering lightning-fast training, reduced memory footprint, and impressive performance gains. With LoRA, achieving optimal model performance is not just efficient—it's a game-changer for AI development."><meta name=twitter:image:src content="lora.jpg"><link rel=canonical href=http://bimaltimilsina.com.np/blog/lora/><link rel=robots href=/robots.txt><meta name=msapplication-TileColor content="#0f172a"><meta name=theme-color content="#0f172a"><meta name=next-head-count content="29"><script src=https://cdn.tailwindcss.com></script><link rel=stylesheet href=http://bimaltimilsina.com.np/css/style.css><script src=https://cdn.jsdelivr.net/npm/js-polyfills@0.1.43/polyfill.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js type=text/javascript></script><script src=https://unpkg.com/scrollreveal/dist/scrollreveal.min.js></script><script src=http://bimaltimilsina.com.np/js/site_main.js defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script></head><body class="bg-slate-900 leading-relaxed text-slate-400 selection:bg-teal-300 selection:text-teal-900"><div class="font-sans group/spotlight relative"><div id=glow_box class="pointer-events-none position-fixed inset-0 z-30 transition duration-300 lg:absolute"></div><main><div class="mx-auto max-w-3xl px-4"><article><div><header class="pt-16 md:pt-20 lg:pt-28 mb-10 md:mb-20"><a class="group mb-2 inline-flex items-center font-semibold leading-tight text-teal-300" href=http://bimaltimilsina.com.np/blog/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor" class="mr-1 h-4 w-4 rotate-180 transition-transform group-hover:-translate-x-2" aria-hidden="true"><path fill-rule="evenodd" d="M3 10a.75.75.0 01.75-.75h10.638L10.23 5.29a.75.75.0 111.04-1.08l5.5 5.25a.75.75.0 010 1.08l-5.5 5.25a.75.75.0 11-1.04-1.08l4.158-3.96H3.75A.75.75.0 013 10z" clip-rule="evenodd"/></svg>Blog</a><h1 class="text-3xl font-semibold tracking-tight leading-tight text-gray-900 md:text-4xl lg:text-4xl dark:text-white">Revolutionizing LLM Fine-Tuning:Low-Rank Adaptation (LoRA)</h1><div class="text-gray-400 mt-6"><time>Apr 17, 2024</time> — <span>13
mins read</span> — <a class="group mb-2 inline-flex items-center font-semibold leading-tight text-teal-300" href=http://bimaltimilsina.com.np/categories/large-language-models>Large Language Models</a></div></header><div class="prose md:prose-lg dark:prose-invert article prose-headings:font-semibold mt-6"><img src=lora.jpg alt><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>Large Language Models (LLMs) and Neural Networks have revolutionized tasks like classification, summarization, and information extraction. However, achieving precision in these tasks often necessitates fine-tuning.</p><p>Traditional fine-tuning involves appending a task-specific head and updating the neural network&rsquo;s weights during training. This approach contrasts with training from scratch, where the model&rsquo;s weights are initialized randomly. In fine-tuning, the weights are already somewhat optimized from the pre-training phase.</p><p>Full fine-tuning entails training all layers of the neural network. While it typically yields superior results, it also demands substantial computational resources and time.</p><p>Nevertheless, efficient fine-tuning methods have emerged, offering promising alternatives. Among these, Low Rank Adaptation (LoRA) stands out for its ability to outperform full fine-tuning in certain scenarios, notably in preventing catastrophic forgetting, where the pre-trained model&rsquo;s knowledge diminishes during fine-tuning.</p><h2 id=lora class="relative group">LORA <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#lora aria-label=Anchor>#</a></span></h2><p>In the training of neural network models, a set of weights is established, denoted as $W$. If the neural network&rsquo;s input is $x$, each layer&rsquo;s output is computed by multiplying the weights with the input and applying a non-linear activation function like ReLU:</p><p>$$
h = \text{ReLU}(Wx)
$$</p><p>Here, $h$ represents the model&rsquo;s output.</p><p><figure><img src=/img/lora/1695672238737.png alt="Neural Network Training" class="mx-auto my-0 rounded-md"></figure></p><p>In traditional fine-tuning, the pre-trained neural network&rsquo;s weights are adjusted to suit a new task. This process involves continuing training from the previous state, resulting in minor changes in the model&rsquo;s weights, denoted as $\Delta{W}$, where the new model&rsquo;s weights become $W + \Delta{W}$.</p><p>However, this method can be resource-intensive for Large Language Models. In LoRA, rather than directly modifying $W$, we decompose the weight matrix to achieve the desired adjustments.</p><figure class=object-center><img src=/img/lora/1_25onX1itf2Wkz8M7FLiXnA.webp alt="Traditional finetuning. Here W is frozen where as ΔW is trainable (Image by Lora Paper author)"><figcaption class="text-center italic">fig. Traditional finetuning. Here W is frozen where as ΔW is trainable (Image by Lora Paper author)</figcaption></figure><h3 id=the-intrinsic-rank-hypothesis class="relative group">The Intrinsic Rank Hypothesis <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-intrinsic-rank-hypothesis aria-label=Anchor>#</a></span></h3><p>Research suggests that the weights of neural networks are overparametrized, implying that not all weight elements are equally crucial. This idea is encapsulated in the intrinsic rank hypothesis.</p><p>The intrinsic rank hypothesis proposes that significant changes in neural networks can be captured using a lower-dimensional representation. When fine-tuning neural networks, these changes in weights can be effectively encapsulated using low-rank matrices, implying that only a subset of the weight changes is essential.</p><h3 id=introducing-matrices-a-and-b class="relative group">Introducing Matrices $A$ and $B$ <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introducing-matrices-a-and-b aria-label=Anchor>#</a></span></h3><p>Based on the intrinsic rank hypothesis, we represent $W$ with smaller matrices. Let&rsquo;s assume that $W$ has dimensions $(d, k)$. We define two smaller matrices, $A$ and $B$, with dimensions $(d, r)$ and $(r, k)$ respectively.</p><p>Here, $r$ represents the rank (reduced dimension) of the matrix, serving as a hyperparameter during model fine-tuning. The number of parameters of LoRA-adapted layers depends on the value of $r$.</p><p>The product of matrices $A$ and $B$ represents the change in pre-trained weights $\Delta{W}$. Thus, the updated weight matrix ($W&rsquo;$) becomes:</p><p>$$
W&rsquo; = W + BA
$$</p><p>where:</p><p>$$
W&rsquo; = W + \Delta{W} = W + BA
$$</p><p>$$
B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}
$$</p><p>$$
r &#171; \min(d, k)
$$</p><p>In this equation, $W$ remains fixed (not updated during fine-tuning), while $A$ and $B$ are lower-dimensional matrices, with their product representing a low-rank approximation of $\Delta{W}$.</p><p>Authors typically initialize $A$ with random Gaussian values and $B$ with zeros, ensuring that $\Delta{W} = BA$ is zero at the beginning of training.</p><p>With LoRA, the output of the neural network becomes:</p><p>$$
h = W_0x + BAx = (W_0 + BA) x
$$</p><h3 id=impact-of-lower-rank-of-trainable-parameters class="relative group">Impact of Lower Rank of Trainable Parameters <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#impact-of-lower-rank-of-trainable-parameters aria-label=Anchor>#</a></span></h3><p>By selecting matrices $A$ and $B$ to have a lower rank $r$, the number of trainable parameters is significantly reduced.</p><p>For instance, if $W$ is a $d \times d$ matrix, traditional weight updating would involve $d^2$ parameters. However, with $B$ and $A$, the total number of parameters reduces to $2dr$, which is much smaller when $r &#171; d$.</p><p>For example, if a model has 60,000 parameters, and the dimension of $W$ is $(300,200)$, matrices $A$ and $B$ will have dimensions $(300, r)$ and $(r, 200)$ respectively.</p><p>If $r = 4$, then the total number of trainable parameters will be $300 \times 4 + 200 \times 4 = 2000$, just 3% of the total parameters.</p><p>Similarly, if $r = 8$, then the total parameters will be 4000, still substantially lower than the original 60,000 of the model.</p><p>As models grow larger, the ratio of trainable to frozen parameters diminishes rapidly. For example, the base GPT-2 (L) model has 774 million trainable parameters, whereas the LoRA-adapted model has only 770 thousand — fewer than 0.1% of the base total.</p><blockquote><p>One of the most impressive aspects of LoRA is that fine-tuned LoRA models typically perform as well as or better than their base model counterparts that have been finetuned.</p></blockquote><p>The reduction in the number of trainable parameters achieved through Low-Rank Adaptation (LoRA) offers several significant benefits, particularly when fine-tuning large-scale neural networks:</p><ol><li><strong>Reduced Memory Footprint:</strong> LoRA decreases memory needs by lowering the number of parameters to update, aiding in the management of large-scale models.</li><li><strong>Faster Training and Adaptation:</strong> By simplifying computational demands, LoRA accelerates the training and fine-tuning of large models for new tasks.</li><li><strong>Feasibility for Smaller Hardware:</strong> LoRA’s lower parameter count enables the fine-tuning of substantial models on less powerful hardware, like modest GPUs or CPUs.</li><li><strong>Scaling to Larger Models:</strong> LoRA facilitates the expansion of AI models without a corresponding increase in computational resources, making the management of growing model sizes more practical.</li></ol><h2 id=implementation-from-scratch-using-python class="relative group">Implementation from Scratch using Python <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation-from-scratch-using-python aria-label=Anchor>#</a></span></h2><p>Now, Let’s implement the LORA method from scratch. You need to have basic knowledge of pytorch to understand the code.</p><p>At first, Let’s import all the required libraries</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>copy</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>from</span> <span style=color:#ff7b72>tqdm</span> <span style=color:#ff7b72>import</span> tqdm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch.nn</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>nn</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torchvision.datasets</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>datasets</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torchvision.transforms</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>transforms</span>
</span></span></code></pre></div><p>Here, we import <code>copy</code> to copy our model without changing the original model.</p><p>All other libraries are from pytorch.</p><p>Next, for deterministic results, let’s set seed to some number , I will use it as 0</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>_ <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>manual_seed(<span style=color:#a5d6ff>0</span>)
</span></span></code></pre></div><p>Now let’s define a function to get the device name.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>get_device</span>():
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> torch<span style=color:#ff7b72;font-weight:700>.</span>cuda<span style=color:#ff7b72;font-weight:700>.</span>is_available():
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>device(<span style=color:#a5d6ff>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>elif</span> torch<span style=color:#ff7b72;font-weight:700>.</span>backends<span style=color:#ff7b72;font-weight:700>.</span>mps<span style=color:#ff7b72;font-weight:700>.</span>is_available():
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>device(<span style=color:#a5d6ff>&#34;mps&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>device(<span style=color:#a5d6ff>&#34;cpu&#34;</span>)
</span></span></code></pre></div><p>We wil use MNIST dataset, for that we download MNIST dataset and create a dataloader.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transform <span style=color:#ff7b72;font-weight:700>=</span> transforms<span style=color:#ff7b72;font-weight:700>.</span>Compose([transforms<span style=color:#ff7b72;font-weight:700>.</span>ToTensor(), transforms<span style=color:#ff7b72;font-weight:700>.</span>Normalize((<span style=color:#a5d6ff>0.1301</span>, ), (<span style=color:#a5d6ff>0.3081</span>, ))])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_trainset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;./data&#34;</span>, train <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>True</span>, download<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_trainset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>, shuffle<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_testset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;./data&#34;</span>, train<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>False</span>, download<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>test_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_testset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#ff7b72;font-weight:700>=</span> get_device()
</span></span></code></pre></div><p>Let’s define a simple neural network to classify the digits.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>DigitsNet</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, input_shape, num_classes):
</span></span><span style=display:flex><span>        super(DigitsNet, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear1 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(input_shape, <span style=color:#a5d6ff>256</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear2 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>256</span>, <span style=color:#a5d6ff>128</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>linear3 <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Linear(<span style=color:#a5d6ff>128</span>,num_classes)
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>relu <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>ReLU()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear1(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>linear3(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff7b72;font-weight:700>=</span> self<span style=color:#ff7b72;font-weight:700>.</span>relu(x)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> x
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#ff7b72;font-weight:700>=</span> DigitsNet(<span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>, <span style=color:#a5d6ff>10</span>)<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span></code></pre></div><p>Then, we create a function to train and test the model using the dataset we downloaded earlier.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>train</span>(train_loader, model, epochs <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>5</span>, limit<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>None</span>):
</span></span><span style=display:flex><span>    ce_loss <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>optim<span style=color:#ff7b72;font-weight:700>.</span>Adam(model<span style=color:#ff7b72;font-weight:700>.</span>parameters(), lr<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>0.001</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    total_iterations <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>for</span> epoch <span style=color:#ff7b72;font-weight:700>in</span> range(epochs):
</span></span><span style=display:flex><span>        model<span style=color:#ff7b72;font-weight:700>.</span>train()
</span></span><span style=display:flex><span>        loss_sum <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>        num_iterations <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>        data_iterator <span style=color:#ff7b72;font-weight:700>=</span> tqdm(train_loader, desc<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Epoch </span><span style=color:#a5d6ff>{</span>epoch<span style=color:#ff7b72;font-weight:700>+</span><span style=color:#a5d6ff>1</span><span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>if</span> limit <span style=color:#ff7b72;font-weight:700>is</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#79c0ff>None</span>:
</span></span><span style=display:flex><span>            data_iterator<span style=color:#ff7b72;font-weight:700>.</span>total <span style=color:#ff7b72;font-weight:700>=</span> limit
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> data <span style=color:#ff7b72;font-weight:700>in</span> data_iterator:
</span></span><span style=display:flex><span>            num_iterations <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>            total_iterations <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>            x, y <span style=color:#ff7b72;font-weight:700>=</span> data
</span></span><span style=display:flex><span>            x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            y <span style=color:#ff7b72;font-weight:700>=</span> y<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            optimizer<span style=color:#ff7b72;font-weight:700>.</span>zero_grad()
</span></span><span style=display:flex><span>            output <span style=color:#ff7b72;font-weight:700>=</span> model(x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>))
</span></span><span style=display:flex><span>            loss <span style=color:#ff7b72;font-weight:700>=</span> ce_loss(output, y)
</span></span><span style=display:flex><span>            loss_sum <span style=color:#ff7b72;font-weight:700>+=</span> loss
</span></span><span style=display:flex><span>            avg_loss <span style=color:#ff7b72;font-weight:700>=</span> loss_sum <span style=color:#ff7b72;font-weight:700>/</span> num_iterations
</span></span><span style=display:flex><span>            data_iterator<span style=color:#ff7b72;font-weight:700>.</span>set_postfix(loss<span style=color:#ff7b72;font-weight:700>=</span>avg_loss<span style=color:#ff7b72;font-weight:700>.</span>item())
</span></span><span style=display:flex><span>            loss<span style=color:#ff7b72;font-weight:700>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#ff7b72;font-weight:700>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>if</span> limit <span style=color:#ff7b72;font-weight:700>is</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#79c0ff>None</span> <span style=color:#ff7b72;font-weight:700>and</span> total_iterations <span style=color:#ff7b72;font-weight:700>&gt;=</span> limit:
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>return</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train(train_loader, model, epochs<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>)
</span></span></code></pre></div><p>OUTPUT:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Epoch 1: 100%|██████████| 6000/6000 [00:47&lt;00:00, 125.35it/s, loss=0.72]
</span></span></code></pre></div><p>Let’t test the results.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>test</span>(model):
</span></span><span style=display:flex><span>    correct <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    total <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>0</span>
</span></span><span style=display:flex><span>    wrong_counts <span style=color:#ff7b72;font-weight:700>=</span> [<span style=color:#a5d6ff>0</span> <span style=color:#ff7b72>for</span> i <span style=color:#ff7b72;font-weight:700>in</span> range(<span style=color:#a5d6ff>10</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>with</span> torch<span style=color:#ff7b72;font-weight:700>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> data <span style=color:#ff7b72;font-weight:700>in</span> tqdm(test_loader, desc<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#34;Testing&#34;</span>):
</span></span><span style=display:flex><span>            x, y <span style=color:#ff7b72;font-weight:700>=</span> data
</span></span><span style=display:flex><span>            x <span style=color:#ff7b72;font-weight:700>=</span> x<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            y <span style=color:#ff7b72;font-weight:700>=</span> y<span style=color:#ff7b72;font-weight:700>.</span>to(device)
</span></span><span style=display:flex><span>            output<span style=color:#ff7b72;font-weight:700>=</span> model(x<span style=color:#ff7b72;font-weight:700>.</span>view(<span style=color:#ff7b72;font-weight:700>-</span><span style=color:#a5d6ff>1</span>, <span style=color:#a5d6ff>28</span><span style=color:#ff7b72;font-weight:700>*</span><span style=color:#a5d6ff>28</span>))
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>for</span> idx, i <span style=color:#ff7b72;font-weight:700>in</span> enumerate(output):
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>if</span> torch<span style=color:#ff7b72;font-weight:700>.</span>argmax(i) <span style=color:#ff7b72;font-weight:700>==</span> y[idx]:
</span></span><span style=display:flex><span>                    correct <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>else</span>:
</span></span><span style=display:flex><span>                    wrong_counts[y[idx]] <span style=color:#ff7b72;font-weight:700>+=</span> <span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>                total<span style=color:#ff7b72;font-weight:700>+=</span><span style=color:#a5d6ff>1</span>
</span></span><span style=display:flex><span>        print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Accuracy: </span><span style=color:#a5d6ff>{</span>round(correct<span style=color:#ff7b72;font-weight:700>/</span>total, <span style=color:#a5d6ff>3</span>) <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>100</span><span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>for</span> i <span style=color:#ff7b72;font-weight:700>in</span> range(len(wrong_counts)):
</span></span><span style=display:flex><span>            print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Wrong counts for the digit </span><span style=color:#a5d6ff>{</span>i<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>: </span><span style=color:#a5d6ff>{</span>wrong_counts[i]<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test(model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:06&lt;00:00, 156.93it/s]
</span></span><span style=display:flex><span>Accuracy: 89.1
</span></span><span style=display:flex><span>Wrong counts for the digit 0: 33
</span></span><span style=display:flex><span>Wrong counts for the digit 1: 12
</span></span><span style=display:flex><span>Wrong counts for the digit 2: 147
</span></span><span style=display:flex><span>Wrong counts for the digit 3: 115
</span></span><span style=display:flex><span>Wrong counts for the digit 4: 69
</span></span><span style=display:flex><span>Wrong counts for the digit 5: 196
</span></span><span style=display:flex><span>Wrong counts for the digit 6: 98
</span></span><span style=display:flex><span>Wrong counts for the digit 7: 91
</span></span><span style=display:flex><span>Wrong counts for the digit 8: 185
</span></span><span style=display:flex><span>Wrong counts for the digit 9: 148
</span></span></code></pre></div><p>Here, you can see the accuracy of model is 89.1%. We can see model is performing badly in case of digits 2, 3, 5, 8 and 9. While finetuning, we finetune the model for these digits.</p><p>Now, We will create another copy of model so that we can check our output later when we finetune the model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>original_model <span style=color:#ff7b72;font-weight:700>=</span>copy<span style=color:#ff7b72;font-weight:700>.</span>deepcopy(model)
</span></span></code></pre></div><p>Before applying, Lora, let’s create a function to get the number of trainable parameters in model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>count_parameters</span>(model):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> sum(p<span style=color:#ff7b72;font-weight:700>.</span>numel() <span style=color:#ff7b72>for</span> p <span style=color:#ff7b72;font-weight:700>in</span> model<span style=color:#ff7b72;font-weight:700>.</span>parameters() <span style=color:#ff7b72>if</span> p<span style=color:#ff7b72;font-weight:700>.</span>requires_grad)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>count_parameters(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#  235,146</span>
</span></span></code></pre></div><p>We have total of 235,146 parameters. Remember this number, we will compare it in later phase.</p><h3 id=method-1-using-parametrization class="relative group">Method 1: Using Parametrization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#method-1-using-parametrization aria-label=Anchor>#</a></span></h3><p>Let’s create a LORA Adapter which we will apply in the model. For simplicity I am using <code>rank=1</code> and <code>alpha = 1.</code></p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>LoraAdapter</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, features_in, features_out, rank<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>, alpha<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>, device<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#34;mps&#34;</span>):
</span></span><span style=display:flex><span>        <span style=color:#8b949e;font-style:italic># features_out : k, features_in: d, weight (d,k) -&gt; (out_features, in_fecatures)</span>
</span></span><span style=display:flex><span>        super(LoraAdapter, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>A <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Parameter(torch<span style=color:#ff7b72;font-weight:700>.</span>zeros((rank, features_out))<span style=color:#ff7b72;font-weight:700>.</span>to(device))
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>B <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Parameter(torch<span style=color:#ff7b72;font-weight:700>.</span>zeros((features_in, rank))<span style=color:#ff7b72;font-weight:700>.</span>to(device))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        nn<span style=color:#ff7b72;font-weight:700>.</span>init<span style=color:#ff7b72;font-weight:700>.</span>normal_(self<span style=color:#ff7b72;font-weight:700>.</span>A, mean<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>0</span>, std<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>) <span style=color:#8b949e;font-style:italic># gaussian random init</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>scale <span style=color:#ff7b72;font-weight:700>=</span> alpha <span style=color:#ff7b72;font-weight:700>/</span> rank
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>enabled <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, original_weights):
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>if</span> self<span style=color:#ff7b72;font-weight:700>.</span>enabled:
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>return</span> original_weights <span style=color:#ff7b72;font-weight:700>+</span> torch<span style=color:#ff7b72;font-weight:700>.</span>matmul(self<span style=color:#ff7b72;font-weight:700>.</span>B, self<span style=color:#ff7b72;font-weight:700>.</span>A)<span style=color:#ff7b72;font-weight:700>.</span>view(original_weights<span style=color:#ff7b72;font-weight:700>.</span>shape) <span style=color:#ff7b72;font-weight:700>*</span> self<span style=color:#ff7b72;font-weight:700>.</span>scale
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>return</span> original_weights
</span></span></code></pre></div><p>Now, I would like to discuss two methods from which we can finetune the model. There is a class parametrize, which helps us in replacing the original weights of any pytorch model. We will first go through this method. Since, this method will help us in getting the results of old model without much hassle.</p><p>So basically, parametrization replaces the parameters of model with some other function, Here in this case, we replace the &ldquo;weight&rdquo; of each layer with our custom function. After registering parametrization, whenever we call this layer, it will return the function we used, not the original weights of model. This will be helpful here since we don&rsquo;t want to touch the original weights of model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>torch.nn.utils.parametrize</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>parametrize</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>for</span> name, layer <span style=color:#ff7b72;font-weight:700>in</span> model<span style=color:#ff7b72;font-weight:700>.</span>named_children():
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> isinstance(layer, nn<span style=color:#ff7b72;font-weight:700>.</span>Linear):
</span></span><span style=display:flex><span>        parametrize<span style=color:#ff7b72;font-weight:700>.</span>register_parametrization(layer, <span style=color:#a5d6ff>&#34;weight&#34;</span>, LoraAdapter(layer<span style=color:#ff7b72;font-weight:700>.</span>in_features, layer<span style=color:#ff7b72;font-weight:700>.</span>out_features, rank<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>2</span>, alpha<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>))
</span></span></code></pre></div><p>Now, remember the model layers after we use parametrization, it adds additional parametrized layer in the model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>DigitsNet(
</span></span><span style=display:flex><span>  (linear1): ParametrizedLinear(
</span></span><span style=display:flex><span>    in_features=784, out_features=256, bias=True
</span></span><span style=display:flex><span>    (parametrizations): ModuleDict(
</span></span><span style=display:flex><span>      (weight): ParametrizationList(
</span></span><span style=display:flex><span>        (0): LoraAdapter()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear2): ParametrizedLinear(
</span></span><span style=display:flex><span>    in_features=256, out_features=128, bias=True
</span></span><span style=display:flex><span>    (parametrizations): ModuleDict(
</span></span><span style=display:flex><span>      (weight): ParametrizationList(
</span></span><span style=display:flex><span>        (0): LoraAdapter()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear3): ParametrizedLinear(
</span></span><span style=display:flex><span>    in_features=128, out_features=10, bias=True
</span></span><span style=display:flex><span>    (parametrizations): ModuleDict(
</span></span><span style=display:flex><span>      (weight): ParametrizationList(
</span></span><span style=display:flex><span>        (0): LoraAdapter()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Here we can see the model has Parametrized layer with <code>LoraAdapter</code> . Now since we are only going to train the A and B, weight matrices, we will freeze all the weights of model.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>for</span> name, param <span style=color:#ff7b72;font-weight:700>in</span> model<span style=color:#ff7b72;font-weight:700>.</span>named_parameters():
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> <span style=color:#a5d6ff>&#34;A&#34;</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#ff7b72;font-weight:700>in</span> name <span style=color:#ff7b72;font-weight:700>and</span> <span style=color:#a5d6ff>&#34;B&#34;</span> <span style=color:#ff7b72;font-weight:700>not</span> <span style=color:#ff7b72;font-weight:700>in</span> name:
</span></span><span style=display:flex><span>        param<span style=color:#ff7b72;font-weight:700>.</span>requires_grad <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>count_parameters(model)
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># 3124</span>
</span></span></code></pre></div><p>Now see, how the number of parameter decreased from 238270 to 3124. Let’s see the percentage of parameters we are going to train.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>count_parameters(model)<span style=color:#ff7b72;font-weight:700>/</span>count_parameters(original_model) <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>100</span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># 1.32</span>
</span></span></code></pre></div><p>So, we are only training 1.32% of original model. This is still a bit high number since our model is really small. For big models, This number will be in the range of 0-1.</p><p>Now, Let’s create finetuning dataset. For that, we will only use digits in which the model performed poor. We then train the model for 200 iterations. I am limiting the number of iterations for simplicity.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>mnist_dataset <span style=color:#ff7b72;font-weight:700>=</span> datasets<span style=color:#ff7b72;font-weight:700>.</span>MNIST(root<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#34;./data&#34;</span>, train<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>, download <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>True</span>, transform<span style=color:#ff7b72;font-weight:700>=</span>transform)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>for</span> digit <span style=color:#ff7b72;font-weight:700>in</span> [<span style=color:#a5d6ff>2</span>,<span style=color:#a5d6ff>3</span>,<span style=color:#a5d6ff>5</span>,<span style=color:#a5d6ff>8</span>,<span style=color:#a5d6ff>9</span>]:
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> digit <span style=color:#ff7b72;font-weight:700>==</span> <span style=color:#a5d6ff>2</span>:
</span></span><span style=display:flex><span>        exclude_indices <span style=color:#ff7b72;font-weight:700>=</span> mnist_dataset<span style=color:#ff7b72;font-weight:700>.</span>targets <span style=color:#ff7b72;font-weight:700>==</span> digit
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>else</span>:
</span></span><span style=display:flex><span>        indices <span style=color:#ff7b72;font-weight:700>=</span> mnist_dataset<span style=color:#ff7b72;font-weight:700>.</span>targets <span style=color:#ff7b72;font-weight:700>==</span> digit
</span></span><span style=display:flex><span>        exclude_indices <span style=color:#ff7b72;font-weight:700>=</span> exclude_indices <span style=color:#ff7b72;font-weight:700>|</span> indices
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mnist_dataset<span style=color:#ff7b72;font-weight:700>.</span>data <span style=color:#ff7b72;font-weight:700>=</span> mnist_dataset<span style=color:#ff7b72;font-weight:700>.</span>data[exclude_indices]
</span></span><span style=display:flex><span>mnist_dataset<span style=color:#ff7b72;font-weight:700>.</span>targets <span style=color:#ff7b72;font-weight:700>=</span> mnist_dataset<span style=color:#ff7b72;font-weight:700>.</span>targets[exclude_indices]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loader <span style=color:#ff7b72;font-weight:700>=</span> torch<span style=color:#ff7b72;font-weight:700>.</span>utils<span style=color:#ff7b72;font-weight:700>.</span>data<span style=color:#ff7b72;font-weight:700>.</span>DataLoader(mnist_dataset, batch_size<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>10</span>, shuffle<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train(train_loader, model, epochs<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>, limit<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>200</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Epoch 1: 100%|█████████▉| 199/200 [00:01&lt;00:00, 101.99it/s, loss=0.329]
</span></span></code></pre></div><p>Here you can see the loss is decreased from 0.72 to 0.32. But the model may have overfitted the data to only the digits we finetuned. Nevertheless, let’s see the results below.</p><p>As I said earlier, using parametrization, we will have an option to enable or disable paremetrization any time. we will create a function to do the same.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>enable_disable_lora</span>(enabled<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>True</span>):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>for</span> layer <span style=color:#ff7b72;font-weight:700>in</span> [model<span style=color:#ff7b72;font-weight:700>.</span>linear1, model<span style=color:#ff7b72;font-weight:700>.</span>linear2, model<span style=color:#ff7b72;font-weight:700>.</span>linear3]:
</span></span><span style=display:flex><span>        layer<span style=color:#ff7b72;font-weight:700>.</span>parametrizations[<span style=color:#a5d6ff>&#34;weight&#34;</span>][<span style=color:#a5d6ff>0</span>]<span style=color:#ff7b72;font-weight:700>.</span>enabled <span style=color:#ff7b72;font-weight:700>=</span> enabled
</span></span></code></pre></div><p>Let’s check the results before finetuning.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>enable_disable_lora(<span style=color:#79c0ff>False</span>)
</span></span><span style=display:flex><span>test(model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:05&lt;00:00, 171.80it/s]
</span></span><span style=display:flex><span>Accuracy: 89.1
</span></span><span style=display:flex><span>Wrong counts for the digit 0: 33
</span></span><span style=display:flex><span>Wrong counts for the digit 1: 12
</span></span><span style=display:flex><span>Wrong counts for the digit 2: 147
</span></span><span style=display:flex><span>Wrong counts for the digit 3: 115
</span></span><span style=display:flex><span>Wrong counts for the digit 4: 69
</span></span><span style=display:flex><span>Wrong counts for the digit 5: 196
</span></span><span style=display:flex><span>Wrong counts for the digit 6: 98
</span></span><span style=display:flex><span>Wrong counts for the digit 7: 91
</span></span><span style=display:flex><span>Wrong counts for the digit 8: 185
</span></span><span style=display:flex><span>Wrong counts for the digit 9: 148
</span></span></code></pre></div><p>Now, After finetuning</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>enable_disable_lora(<span style=color:#79c0ff>True</span>)
</span></span><span style=display:flex><span>test(model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:07&lt;00:00, 136.35it/s]
</span></span><span style=display:flex><span>Accuracy: 68.30000000000001
</span></span><span style=display:flex><span>Wrong counts for the digit 0: 543
</span></span><span style=display:flex><span>Wrong counts for the digit 1: 484
</span></span><span style=display:flex><span>Wrong counts for the digit 2: 79
</span></span><span style=display:flex><span>Wrong counts for the digit 3: 101
</span></span><span style=display:flex><span>Wrong counts for the digit 4: 609
</span></span><span style=display:flex><span>Wrong counts for the digit 5: 98
</span></span><span style=display:flex><span>Wrong counts for the digit 6: 348
</span></span><span style=display:flex><span>Wrong counts for the digit 7: 765
</span></span><span style=display:flex><span>Wrong counts for the digit 8: 103
</span></span><span style=display:flex><span>Wrong counts for the digit 9: 44
</span></span></code></pre></div><p>As you can see the model is performing much better for the digits we finetuned, but as suspected the model has overfitted the data. So you have to be careful when finetuning any models.</p><h3 id=method-2-without-parametrization class="relative group">Method 2: Without parametrization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#method-2-without-parametrization aria-label=Anchor>#</a></span></h3><p>Let’s copy the original model so that we can modify it and finetune it.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lora_model <span style=color:#ff7b72;font-weight:700>=</span> copy<span style=color:#ff7b72;font-weight:700>.</span>deepcopy(original_model)
</span></span></code></pre></div><p>Let’s create a <code>LoraLayer</code> This layer performs the LORA calculations and returns the changed weights.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>LoraLayer</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, features_in, features_out, rank<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>2</span>, alpha<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>, device<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#34;mps&#34;</span>):
</span></span><span style=display:flex><span>        <span style=color:#8b949e;font-style:italic># features_out : k, features_in: d, weight (d,k) -&gt; (out_features, in_features)</span>
</span></span><span style=display:flex><span>        super(LoraLayer, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>A <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Parameter(torch<span style=color:#ff7b72;font-weight:700>.</span>zeros((features_in, rank))<span style=color:#ff7b72;font-weight:700>.</span>to(device))
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>B <span style=color:#ff7b72;font-weight:700>=</span> nn<span style=color:#ff7b72;font-weight:700>.</span>Parameter(torch<span style=color:#ff7b72;font-weight:700>.</span>zeros((rank, features_out))<span style=color:#ff7b72;font-weight:700>.</span>to(device))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        nn<span style=color:#ff7b72;font-weight:700>.</span>init<span style=color:#ff7b72;font-weight:700>.</span>normal_(self<span style=color:#ff7b72;font-weight:700>.</span>A, mean<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>0</span>, std<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>) <span style=color:#8b949e;font-style:italic># gaussian random init</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>scale <span style=color:#ff7b72;font-weight:700>=</span> alpha <span style=color:#ff7b72;font-weight:700>/</span> rank
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> torch<span style=color:#ff7b72;font-weight:700>.</span>matmul(x, torch<span style=color:#ff7b72;font-weight:700>.</span>matmul(self<span style=color:#ff7b72;font-weight:700>.</span>A, self<span style=color:#ff7b72;font-weight:700>.</span>B)) <span style=color:#ff7b72;font-weight:700>*</span> self<span style=color:#ff7b72;font-weight:700>.</span>scale
</span></span></code></pre></div><p>As now we need to apply LORA layer to all of our linear layers in model and then add the weight with the original weights when inference, we create another model with all of that. I am using <code>rank=2</code> here. To check how rank impacts in finetuning.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>class</span> <span style=color:#f0883e;font-weight:700>LinearwithLORA</span>(nn<span style=color:#ff7b72;font-weight:700>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> __init__(self, layer, rank<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>2</span>, alpha<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>):
</span></span><span style=display:flex><span>        super(LinearwithLORA, self)<span style=color:#ff7b72;font-weight:700>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>layer <span style=color:#ff7b72;font-weight:700>=</span> layer
</span></span><span style=display:flex><span>        self<span style=color:#ff7b72;font-weight:700>.</span>lora <span style=color:#ff7b72;font-weight:700>=</span> LoraLayer(layer<span style=color:#ff7b72;font-weight:700>.</span>in_features, layer<span style=color:#ff7b72;font-weight:700>.</span>out_features, rank<span style=color:#ff7b72;font-weight:700>=</span>rank, alpha<span style=color:#ff7b72;font-weight:700>=</span>alpha)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span> self<span style=color:#ff7b72;font-weight:700>.</span>layer(x) <span style=color:#ff7b72;font-weight:700>+</span> self<span style=color:#ff7b72;font-weight:700>.</span>lora(x)
</span></span></code></pre></div><p>This layers simply adds the lora weights and original weights to get the results.</p><p>Let’s assign each of our linear layers the lora adapter.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>for</span> name, layer <span style=color:#ff7b72;font-weight:700>in</span> lora_model<span style=color:#ff7b72;font-weight:700>.</span>named_children():
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> isinstance(layer, nn<span style=color:#ff7b72;font-weight:700>.</span>Linear):
</span></span><span style=display:flex><span>        setattr(lora_model, name, LinearwithLORA(layer))
</span></span></code></pre></div><p>After replacing Linear layers with <code>LoraLayer</code> we will have the following model:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>DigitsNet(
</span></span><span style=display:flex><span>  (linear1): LinearwithLORA(
</span></span><span style=display:flex><span>    (layer): Linear(in_features=784, out_features=256, bias=True)
</span></span><span style=display:flex><span>    (lora): LoraLayer()
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear2): LinearwithLORA(
</span></span><span style=display:flex><span>    (layer): Linear(in_features=256, out_features=128, bias=True)
</span></span><span style=display:flex><span>    (lora): LoraLayer()
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (linear3): LinearwithLORA(
</span></span><span style=display:flex><span>    (layer): Linear(in_features=128, out_features=10, bias=True)
</span></span><span style=display:flex><span>    (lora): LoraLayer()
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (relu): ReLU()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Here we can see our linear model is replaced by <code>LinearwithLORA.</code></p><p>Let’s freeze all the previous parameters of original model so that we only train the new parameters.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>for</span> layer <span style=color:#ff7b72;font-weight:700>in</span> lora_model<span style=color:#ff7b72;font-weight:700>.</span>children():
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> isinstance(layer, LinearwithLORA):
</span></span><span style=display:flex><span>        layer<span style=color:#ff7b72;font-weight:700>.</span>layer<span style=color:#ff7b72;font-weight:700>.</span>weight<span style=color:#ff7b72;font-weight:700>.</span>requires_grad <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>False</span>
</span></span></code></pre></div><p>Let’s see the number of parameters now:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>count_parameters(lora_model) <span style=color:#8b949e;font-style:italic># 3518</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>count_parameters(lora_model) <span style=color:#ff7b72;font-weight:700>/</span> count_parameters(original_model) <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>100</span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># 1.49 %</span>
</span></span></code></pre></div><p>As we increased the rank of A and B to 2, the parameters also increased by certain amount.</p><p>Now Let’s finetune the model in the same dataset we used earlier.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train(train_loader, lora_model, epochs<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>1</span>, limit<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>200</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Epoch 1: 100%|█████████▉| 199/200 [00:02&lt;00:00, 84.53it/s, loss=0.413]
</span></span></code></pre></div><p>And finally, let’s see the results.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>test(lora_model)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Testing: 100%|██████████| 1000/1000 [00:07&lt;00:00, 138.74it/s]
</span></span><span style=display:flex><span>Accuracy: 78.0
</span></span><span style=display:flex><span>Wrong counts for the digit 0: 88
</span></span><span style=display:flex><span>Wrong counts for the digit 1: 503
</span></span><span style=display:flex><span>Wrong counts for the digit 2: 97
</span></span><span style=display:flex><span>Wrong counts for the digit 3: 101
</span></span><span style=display:flex><span>Wrong counts for the digit 4: 361
</span></span><span style=display:flex><span>Wrong counts for the digit 5: 119
</span></span><span style=display:flex><span>Wrong counts for the digit 6: 120
</span></span><span style=display:flex><span>Wrong counts for the digit 7: 651
</span></span><span style=display:flex><span>Wrong counts for the digit 8: 103
</span></span><span style=display:flex><span>Wrong counts for the digit 9: 55
</span></span></code></pre></div><p>As we can see, it did perform better than the previous method. This is due to the rank. we used rank 2 matrix here. While finetuning any models, you need to find the best value of rank and use that.</p><p>As with previous method, this method also overfitted the data, but the accuracy has improved by 10% which is huge.</p><h2 id=references class="relative group">References <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#references aria-label=Anchor>#</a></span></h2><ul><li><a href=https://arxiv.org/pdf/2106.09685 target=_blank rel=noreferrer>LoRA: Low-Rank Adaptation of Large Language Models</a></li><li><a href=https://www.linkedin.com/pulse/more-efficient-finetuning-implementing-lora-from-scratch-george-davis/ target=_blank rel=noreferrer>More Efficient Finetuning: Implementing LoRA from Scratch</a></li><li><a href="https://www.youtube.com/watch?v=PXWYUTMt-AU" target=_blank rel=noreferrer>LoRA: Low-Rank Adaptation of Large Language Models - Explained visually + PyTorch code from scratch</a></li><li><a href=https://pytorch.org/tutorials/intermediate/parametrizations.html target=_blank rel=noreferrer>Parametrizations Tutorial</a></li></ul></div></div></article></div><footer><div class="mx-auto max-w-3xl px-4 flex justify-between py-8 md:pt-40 md:pb-20"><a href=/ aria-label=Home><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 79 79"><g transform="translate(-502 -524)"><rect width="79" height="79" rx="14" transform="translate(502 524)" fill="#14b8a6"/><text transform="translate(521 587)" fill="#0f172a" font-size="63" font-family="Geist" font-weight="700"><tspan x="0" y="0">B</tspan></text></g></svg></a><div class=text-right><nav class="text-sm md:text-base"><ul class="flex flex-col gap-4 md:gap-8 sm:flex-row justify-end"><li><a href=https://github.com/timilsinabimal/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Github</a></li><li><a href=https://linkedin.com/in/TimilsinaBimal/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>LinkedIn</a></li><li><a href=https://instagram.com/__bimal_/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Instagram</a></li><li><a href=https://twitter.com/TimilsinaBml/ class="underline-offset-2 hover:underline-offset-4 transition-all text-gray-600 block dark:text-gray-400 dark:hover:text-white !no-underline hover:!underline" target=_blank>Twitter</a></li></ul></nav><span class="text-gray-500 mt-4 block sm:mt-2 text-sm">&copy;2024 Bimal Timilisna</span></div></div></footer><button id=to-top-button onclick=scrollToTop() title="Go To Top" class="hidden fixed z-50 bottom-10 right-10 lg:bottom:20 lg:right:20 sm:bottom:5 sm:right:5 md:bottom:5 md:right:5 p-3 border-0 w-11 h-11 rounded-full shadow-md bg-teal-300 hover:bg-teal-500 text-white text-lg font-semibold transition-colors duration-300">
<svg clip-rule="evenodd" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m18.787 9.473s-4.505-4.502-6.259-6.255c-.147-.146-.339-.22-.53-.22-.192.0-.384.074-.531.22C9.714 4.971 5.211 9.47 5.211 9.47c-.147.147-.219.339-.217.532.001.19.075.38.221.525.292.293.766.295 1.056.004l4.977-4.976v14.692c0 .414.336.75.75.75.413.0.75-.336.75-.75V5.555l4.978 4.978c.289.29.762.287 1.055-.006.145-.145.219-.335.221-.525.002-.192-.07-.384-.215-.529z" fill-rule="nonzero"/></svg>
<span class=sr-only>Go to top</span></button></main></div></body></html>